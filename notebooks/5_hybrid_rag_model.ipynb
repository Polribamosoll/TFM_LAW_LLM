{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid RAG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in /usr/local/lib/python3.9/dist-packages (4.28.3)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.9/dist-packages (0.22.2)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.9/dist-packages (0.29.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (4.66.2)\n",
      "Requirement already satisfied: langchain-pinecone in /usr/local/lib/python3.9/dist-packages (0.1.0)\n",
      "Requirement already satisfied: xformers in /usr/local/lib/python3.9/dist-packages (0.0.25.post1)\n",
      "Requirement already satisfied: nomic in /usr/local/lib/python3.9/dist-packages (3.0.25)\n",
      "Requirement already satisfied: gradio-client==0.16.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (0.16.0)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (3.10.1)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.9/dist-packages (from gradio) (5.8.0)\n",
      "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (1.23.1)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (3.5.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.9/dist-packages (from gradio) (0.12.3)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (2.1.1)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (9.2.0)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/lib/python3/dist-packages (from gradio) (5.3.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from gradio) (23.2)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (5.3.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (4.11.0)\n",
      "Collecting urllib3~=2.0\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.9/dist-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.9/dist-packages (from gradio) (0.3.2)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (1.4.3)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (0.29.0)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.9/dist-packages (from gradio) (0.0.9)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.9/dist-packages (from gradio) (0.27.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.9/dist-packages (from gradio) (2.7.1)\n",
      "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from gradio) (0.4.2)\n",
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.9/dist-packages (from gradio) (0.110.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from gradio-client==0.16.0->gradio) (2024.3.1)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.9/dist-packages (from gradio-client==0.16.0->gradio) (11.0.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub) (2.31.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub) (3.7.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate) (5.9.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (2.2.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.9/dist-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.40 in /usr/local/lib/python3.9/dist-packages (from langchain-pinecone) (0.1.46)\n",
      "Requirement already satisfied: pinecone-client<4.0.0,>=3.2.2 in /usr/local/lib/python3.9/dist-packages (from langchain-pinecone) (3.2.2)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (2.8.4)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.9/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nomic) (8.1.3)\n",
      "Requirement already satisfied: loguru in /usr/local/lib/python3.9/dist-packages (from nomic) (0.7.2)\n",
      "Requirement already satisfied: jsonlines in /usr/local/lib/python3.9/dist-packages (from nomic) (4.0.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.9/dist-packages (from nomic) (13.7.1)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.9/dist-packages (from nomic) (8.0.0)\n",
      "Requirement already satisfied: pyjwt in /usr/local/lib/python3.9/dist-packages (from nomic) (2.8.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.9/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.7.2)\n",
      "Requirement already satisfied: toolz in /usr/local/lib/python3.9/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx>=0.24.1->gradio) (2019.11.28)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.9/dist-packages (from httpx>=0.24.1->gradio) (1.2.0)\n",
      "Requirement already satisfied: idna in /usr/lib/python3/dist-packages (from httpx>=0.24.1->gradio) (2.8)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.9/dist-packages (from httpx>=0.24.1->gradio) (3.6.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.9/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.9/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources<7.0,>=1.3->gradio) (3.8.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.9/dist-packages (from langchain-core<0.2.0,>=0.1.40->langchain-pinecone) (8.2.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.9/dist-packages (from langchain-core<0.2.0,>=0.1.40->langchain-pinecone) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from langchain-core<0.2.0,>=0.1.40->langchain-pinecone) (0.1.51)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib~=3.0->gradio) (4.34.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib~=3.0->gradio) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib~=3.0->gradio) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib~=3.0->gradio) (1.4.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas<3.0,>=1.0->gradio) (2022.1)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.9/dist-packages (from pydantic>=2.0->gradio) (2.18.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from typer<1.0,>=0.12->gradio) (1.4.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from rich->nomic) (2.17.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from rich->nomic) (3.0.0)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.9/dist-packages (from fastapi->gradio) (0.37.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.9/dist-packages (from jsonlines->nomic) (23.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub) (2.1.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.9/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.40->langchain-pinecone) (2.4)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py>=2.2.0->rich->nomic) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.14.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Installing collected packages: urllib3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.6\n",
      "    Uninstalling urllib3-1.26.6:\n",
      "      Successfully uninstalled urllib3-1.26.6\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradient 2.0.5 requires attrs<=19, but you have attrs 23.2.0 which is incompatible.\n",
      "gradient 2.0.5 requires marshmallow<3.0, but you have marshmallow 3.21.1 which is incompatible.\n",
      "fastai 2.7.9 requires torch<1.14,>=1.7, but you have torch 2.2.2 which is incompatible.\n",
      "botocore 1.27.27 requires urllib3<1.27,>=1.25.4, but you have urllib3 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed urllib3-2.2.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gradio huggingface-hub accelerate tqdm langchain-pinecone xformers accelerate nomic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.9/dist-packages (0.22.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface_hub) (4.11.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface_hub) (3.7.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.9/dist-packages (from huggingface_hub) (2024.3.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface_hub) (5.3.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.9/dist-packages (from huggingface_hub) (4.66.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface_hub) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface_hub) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface_hub) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface_hub) (2.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement stack (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for stack\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.40.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.23.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.2.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple/\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.9/dist-packages (0.43.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from bitsandbytes) (2.2.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from bitsandbytes) (1.23.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from torch->bitsandbytes) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.9/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.9/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.9/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.9/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.9/dist-packages (from torch->bitsandbytes) (2.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->bitsandbytes) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from torch->bitsandbytes) (4.11.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.9/dist-packages (from torch->bitsandbytes) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.9/dist-packages (from torch->bitsandbytes) (2.19.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->bitsandbytes) (2.8.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.9/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch->bitsandbytes) (3.7.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.9/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->bitsandbytes) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -i https://pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting urllib3==1.26.6\n",
      "  Using cached urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
      "Installing collected packages: urllib3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.1\n",
      "    Uninstalling urllib3-2.2.1:\n",
      "      Successfully uninstalled urllib3-2.2.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradio 4.28.3 requires urllib3~=2.0, but you have urllib3 1.26.6 which is incompatible.\n",
      "gradient 2.0.5 requires attrs<=19, but you have attrs 23.2.0 which is incompatible.\n",
      "gradient 2.0.5 requires marshmallow<3.0, but you have marshmallow 3.21.1 which is incompatible.\n",
      "fastai 2.7.9 requires torch<1.14,>=1.7, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed urllib3-1.26.6\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install urllib3==1.26.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.13 (main, May 23 2022, 22:01:06) \n",
      "[GCC 9.4.0]\n"
     ]
    }
   ],
   "source": [
    "# Python version\n",
    "import sys \n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Variables\n",
    "#from dotenv import load_dotenv\n",
    "import yaml\n",
    "import os\n",
    "import tiktoken\n",
    "\n",
    "# Load env\n",
    "#load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch config\n",
    "from torch import cuda, bfloat16, float16\n",
    "import torch\n",
    "\n",
    "# Torch options\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters from YAML file\n",
    "import os\n",
    "\n",
    "# Change the current working directory to the directory containing the YAML file\n",
    "os.chdir('/notebooks/TFM/TFM_LAW_LLM')\n",
    "\n",
    "# Load parameters from YAML file\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use optimum\n",
    "use_optimum = config[\"use_optimum\"]\n",
    "\n",
    "# Show\n",
    "use_optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://colab.research.google.com/drive/1rt318Ew-5dDw21YZx2zK2vnxbsuDAchH?usp=sharing#scrollTo=YFw8HWIyTCnJ\n",
    "- https://www.reddit.com/r/LocalLLaMA/comments/16j624z/some_questions_of_implementing_llm_to_generate_qa\n",
    "- https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1\n",
    "- https://towardsdatascience.com/rag-how-to-talk-to-your-data-eaf5469b83b0\n",
    "- https://github.com/edumunozsala/question-answering-pinecone-sts\n",
    "- https://medium.com/@pankaj_pandey/fine-tuning-rag-models-for-custom-content-generation-849d7ffce97f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/notebooks/TFM/TFM_LAW_LLM'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set directory to file location\n",
    "from pathlib import Path\n",
    "import sys\n",
    "notebook_location = Path(os.path.abspath(''))\n",
    "os.chdir(notebook_location)\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "current_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for display and visualization\n",
    "from IPython.display import Markdown, display\n",
    "import gradio as gr\n",
    "\n",
    "# Libraries for managing data and serialization\n",
    "import pinecone\n",
    "import yaml\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# General utility libraries\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Libraries related to HuggingFace\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Libraries related to Transformers\n",
    "from transformers import BitsAndBytesConfig\n",
    "from sentence_transformers import CrossEncoder\n",
    "from typing import List\n",
    "#import accelerate\n",
    "\n",
    "# Libraries related to Langchain\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.schema import AIMessage, HumanMessage\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import (\n",
    "    SimpleSequentialChain, \n",
    "    RetrievalQA, \n",
    "    LLMChain,\n",
    "    RetrievalQAWithSourcesChain,\n",
    "    ConversationalRetrievalChain\n",
    ")\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate\n",
    ")\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# Libraries related to Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore  \n",
    "from pinecone import Pinecone\n",
    "\n",
    "# Libraries related to optimization\n",
    "import xformers\n",
    "\n",
    "# Other miscellaneous libraries\n",
    "from tqdm.notebook import tqdm\n",
    "from nomic import atlas\n",
    "import nomic\n",
    "\n",
    "# Local custom functions\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA RTX A4000\n",
      "Memory Usage:\n",
      "Allocated:    0.0 GB\n",
      "Cached:       0.0 GB\n",
      "Available:   15.7 GB\n",
      "Total:       15.7 GB\n"
     ]
    }
   ],
   "source": [
    "# Setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "# CUDA information\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    allocated_memory = torch.cuda.memory_allocated(0) / (1024**3)  # Convert bytes to GB\n",
    "    cached_memory = torch.cuda.memory_reserved(0) / (1024**3)  # Convert bytes to GB\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Convert bytes to GB\n",
    "    available_memory = total_memory - cached_memory\n",
    "    print('Allocated:   ', round(allocated_memory, 1), 'GB')\n",
    "    print('Cached:      ', round(cached_memory, 1), 'GB')\n",
    "    print('Available:  ', round(available_memory, 1), 'GB')\n",
    "    print('Total:      ', round(total_memory, 1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get Pinecone vector store ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 768,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 1068}},\n",
       " 'total_vector_count': 1068}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init pinecone\n",
    "pinecone = Pinecone(api_key = \"03b29f67-c297-4462-825b-13ce23b3d577\")\n",
    "\n",
    "pc = Pinecone(api_key = pinecone)\n",
    "# Connect\n",
    "index_name = 'lawllm-unstructured-database'\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "# Index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sentence-transformers/multi-qa-mpnet-base-cos-v1'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model ID\n",
    "embed_model_id = config[\"embedding_model\"]\n",
    "\n",
    "# Show\n",
    "embed_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/multi-qa-mpnet-base-cos-v1', cache_folder=None, model_kwargs={'device': device(type='cuda')}, encode_kwargs={'device': device(type='cuda'), 'batch_size': 32}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embed model\n",
    "embed_model = HuggingFaceEmbeddings(\n",
    "    model_name = embed_model_id,\n",
    "    model_kwargs = {'device': device},\n",
    "    encode_kwargs = {'device': device, 'batch_size': 32}\n",
    ") \n",
    "\n",
    "# Show\n",
    "embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA RTX A4000\n",
      "Memory Usage:\n",
      "Allocated:    0.0 GB\n",
      "Cached:       0.0 GB\n",
      "Available:   15.7 GB\n",
      "Total:       15.7 GB\n"
     ]
    }
   ],
   "source": [
    "# CUDA information\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    allocated_memory = torch.cuda.memory_allocated(0) / (1024**3)  # Convert bytes to GB\n",
    "    cached_memory = torch.cuda.memory_reserved(0) / (1024**3)  # Convert bytes to GB\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Convert bytes to GB\n",
    "    available_memory = total_memory - cached_memory\n",
    "    print('Allocated:   ', round(allocated_memory, 1), 'GB')\n",
    "    print('Cached:      ', round(cached_memory, 1), 'GB')\n",
    "    print('Available:  ', round(available_memory, 1), 'GB')\n",
    "    print('Total:      ', round(total_memory, 1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model ID\n",
    "use_quantization = config[\"use_quantization\"]\n",
    "\n",
    "# Show\n",
    "use_quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mistralai/Mistral-7B-Instruct-v0.2'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select model\n",
    "model_id = config[\"model\"]\n",
    "\n",
    "# Show\n",
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Your Hugging Face API token\n",
    "api_token = \"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\"\n",
    "\n",
    "# Model identifier\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Load tokenizer with authentication\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    token=api_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set BNB configuration if quantization is enabled\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ") if use_quantization else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a8d630c802492c94117d7585ca7f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set model\n",
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    token=api_token,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA RTX A4000\n",
      "Memory Usage:\n",
      "Allocated:    0.0 GB\n",
      "Cached:       0.0 GB\n",
      "Available:   15.7 GB\n",
      "Total:       15.7 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# CUDA information\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    allocated_memory = torch.cuda.memory_allocated(0) / (1024**3)  # Convert bytes to GB\n",
    "    cached_memory = torch.cuda.memory_reserved(0) / (1024**3)  # Convert bytes to GB\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Convert bytes to GB\n",
    "    available_memory = total_memory - cached_memory\n",
    "    print('Allocated:   ', round(allocated_memory, 1), 'GB')\n",
    "    print('Cached:      ', round(cached_memory, 1), 'GB')\n",
    "    print('Available:  ', round(available_memory, 1), 'GB')\n",
    "    print('Total:      ', round(total_memory, 1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now import the pre_prompt and the prompt_context from the yaml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pre-prompt\n",
    "pre_prompt = config[\"pre_prompt\"]\n",
    "\n",
    "# Create prompt context\n",
    "prompt_context = config[\"prompt_context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General template\n",
    "general_template = pre_prompt + prompt_context + \"A continuación se proporciona el contexto: {context}\" + \" \" + \"pregunta: {query}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mistral template\n",
    "mistral_template = \"<s>[INST]\" + pre_prompt + prompt_context +  \"A continuación se proporciona el contexto: [/INST] {context}\" + \"</s>\" + \"[INST] pregunta: {query} [/INST]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google template\n",
    "google_template = f\"\"\"\n",
    "<start_of_turn>user\n",
    "{pre_prompt}. {prompt_context} A continuación se proporciona el contexto: \n",
    "Contexto: {{context}} \n",
    "Pregunta: {{query}}\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\n",
    "Respuesta: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistral template selected.\n"
     ]
    }
   ],
   "source": [
    "# Define the final template selection logic\n",
    "if \"mistral\" in model_id:\n",
    "    template = mistral_template\n",
    "    selected_template_message = \"Mistral template selected.\"\n",
    "elif \"google\" in model_id:\n",
    "    template = google_template\n",
    "    selected_template_message = \"Google template selected.\"\n",
    "else:\n",
    "    template = general_template\n",
    "    selected_template_message = \"Default template selected.\"\n",
    "\n",
    "# Print out the selected template message\n",
    "print(selected_template_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Template\n",
    "prompt = PromptTemplate(\n",
    "    template = template, \n",
    "    input_variables = [\"context\", \"query\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now print the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'query'], template='<s>[INST]Eres un asistente experto en derecho y leyes españolas y tu objetivo es proporcionar respuestas exhaustivas y precisas a las preguntas planteadas por tus clientes.\\nAsegúrate de basar tus respuestas en el contexto proporcionado, utilizando todas las leyes y normativas relevantes para fundamentar tus argumentos.\\nEs crucial que todas las respuestas estén redactadas en español y presentadas de forma clara y coherente.\\nConsidera ofrecer ejemplos o casos hipotéticos para ilustrar tus puntos de vista.\\nA continuación, se presenta la información relevante que debes usar para responder a las consultas de los clientes. \\nEn caso de no encontrar la respuesta, debes indicarlo de forma explícita.\\nA continuación se proporciona el contexto: [/INST] {context}</s>[INST] pregunta: {query} [/INST]')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LLM Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the LLM Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline with parameters from config file\n",
    "generate_text = transformers.pipeline(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    task = 'text-generation',\n",
    "    return_full_text = config[\"return_full_text\"],\n",
    "    max_new_tokens = config[\"max_new_tokens\"],\n",
    "    repetition_penalty = config[\"repetition_penalty\"],\n",
    "    temperature = config[\"temperature\"],\n",
    "    pad_token_id = tokenizer.eos_token_id,\n",
    "    batch_size = 1\n",
    ")\n",
    "\n",
    "# HF pipeline\n",
    "llm = HuggingFacePipeline(pipeline = generate_text)\n",
    "\n",
    "# Create llm chain \n",
    "llm_chain = LLMChain(llm = llm, prompt = prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x7f2862ba2d60>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text field\n",
    "text_field = \"text\"  \n",
    "\n",
    "# Vector store\n",
    "vectorstore = PineconeVectorStore(index, embed_model, text_field)  \n",
    "\n",
    "# Show\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple context\n",
    "context = \"Eres una API con conocimientos legales. Debes responder a preguntas en Español. Si no conoces la respuesta, admítelo.\"\n",
    "\n",
    "# Query\n",
    "query = 'Explícame el Artículo 245 del Código Penal de España referente a ocupaciones ilegales de bienes inmuebles'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find closer docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see the closer docs to the query and it's scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'context': 'De la acusación y denuncia falsas y de la\\nsimulación de delitos\\n\\nArtículo 456\\n1. Los que, con conocimiento de su falsedad o temerario desprecio hacia la verdad, imputaren a alguna persona hechos\\nque, de ser ciertos, constituirían infracción penal, si esta\\n295',\n",
       "  'score': 0.664225459},\n",
       " {'context': 'Trata de seres humanos, 177 bis.\\nTratos degradantes, 173 a 177.\\nTregua:\\nViolación, 593.\\nTribunal Constitucional:\\nAtentado contra sus miembros, 550.\\nCalumnias, injurias o amenazas contra, 504.\\nTribunal de Cuentas:\\nObstáculos a la investigación, 502.2.\\nTribunal Superior de Justicia:\\nCalumnias, injurias o amenazas contra, 504.\\nTribunal Supremo:\\nCalumnias, injurias o amenazas contra, 504.\\nTribunales:\\nPerturbación del orden, 558.\\nTutela, 120, 192 y 440.\\nInhabilitación para el ejercicio de, 46.\\n\\nU\\nUltrajes:\\nA España, Comunidades Autónomas, símbolos o emblemas, 543.\\nUrbanismo:\\nDelitos contra, 319 y 320.\\nUsurpación:\\nAlteración de términos o lindes, 246.\\nDistracción del curso de aguas, 247.\\nOcupación de inmuebles, 245.\\nUsurpación de atribuciones, 506 a 509.\\nUsurpación del estado civil, 401.\\n429\\n\\n\\x0cUsurpación de funciones públicas, 402.',\n",
       "  'score': 0.644809961},\n",
       " {'context': 'JUAN CARLOS I\\nRey de España\\nA todos los que la presente vieren y entendieren.\\nSabed: Que las Cortes Generales han aprobado y Yo vengo en\\nsancionar la siguiente Ley Orgánica:\\nEXPOSICIÓN DE MOTIVOS\\nSi se ha llegado a definir el ordenamiento jurídico como conjunto de normas que regulan el uso de la fuerza, puede entenderse fácilmente la importancia del Código Penal en cualquier\\nsociedad civilizada. El Código Penal define los delitos y faltas\\nque constituyen los presupuestos de la aplicación de la forma\\nsuprema que puede revestir el poder coactivo del Estado: la\\npena criminal. En consecuencia, ocupa un lugar preeminente en\\nel conjunto del ordenamiento, hasta el punto de que, no sin\\nrazón, se ha considerado como una especie de «Constitución\\nnegativa». El Código Penal ha de tutelar los valores y principios\\nbásicos de la convivencia social. Cuando esos valores y principios cambian, debe también cambiar. En nuestro país, sin\\nembargo, pese a las profundas modificaciones de orden social,',\n",
       "  'score': 0.642645717}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarity output\n",
    "similarity_output = vectorstore.similarity_search_with_score(query, k = config['top_k_docs'])\n",
    "\n",
    "# Context preprocessed\n",
    "context_processed = [{\"context\": doc.page_content, \"score\": score} for doc, score in similarity_output]\n",
    "\n",
    "# Show\n",
    "context_processed[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cross-encoder/ms-marco-MiniLM-L-6-v2'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model ID\n",
    "reranking_model = config[\"reranking_model\"]\n",
    "\n",
    "# Show\n",
    "reranking_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model ID\n",
    "top_reranked_docs = config[\"top_reranked_docs\"]\n",
    "\n",
    "# Show\n",
    "top_reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting 'title' keys\n",
    "final_context = [entry['context'] for entry in context_processed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sentence_transformers.cross_encoder.CrossEncoder.CrossEncoder at 0x7f29bc6215e0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross encoder\n",
    "cross_encoder = CrossEncoder(reranking_model)\n",
    "\n",
    "# Show\n",
    "cross_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA RTX A4000\n",
      "Memory Usage:\n",
      "Allocated:    0.4 GB\n",
      "Cached:       0.5 GB\n",
      "Available:   15.3 GB\n",
      "Total:       15.7 GB\n"
     ]
    }
   ],
   "source": [
    "# CUDA information\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    allocated_memory = torch.cuda.memory_allocated(0) / (1024**3)  # Convert bytes to GB\n",
    "    cached_memory = torch.cuda.memory_reserved(0) / (1024**3)  # Convert bytes to GB\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Convert bytes to GB\n",
    "    available_memory = total_memory - cached_memory\n",
    "    print('Allocated:   ', round(allocated_memory, 1), 'GB')\n",
    "    print('Cached:      ', round(cached_memory, 1), 'GB')\n",
    "    print('Available:  ', round(available_memory, 1), 'GB')\n",
    "    print('Total:      ', round(total_memory, 1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents(cross_encoder, text_field, query:str, retrieved_documents:List[dict]):\n",
    "    \"\"\"\n",
    "    Ranks retrieved documents based on their relevance to a given query using a cross-encoder model.\n",
    "\n",
    "    Parameters:\n",
    "    - cross_encoder (CrossEncoder): A cross-encoder model from the sentence-transformers library.\n",
    "    - query (str): The query string for which the documents are to be ranked.\n",
    "    - retrieved_documents (List[dict]): A list of dictionaries representing documents. Each dictionary should have a 'text' field\n",
    "      containing the document text and any additional fields that you want to retain in the output dictionary.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary where the key is the rank position (starting from 0 for the most relevant document)\n",
    "      and the value is a dictionary containing the document text and any additional fields. The documents are ranked\n",
    "      in descending order of relevance to the query.\n",
    "\n",
    "    Usage:\n",
    "    ranked_docs = rank_documents(cross_encoder, query, retrieved_documents)\n",
    "\n",
    "    Note: This function requires the sentence-transformers library and a pretrained cross-encoder model.\n",
    "    \"\"\"\n",
    "    pairs = [[query, doc[text_field]] for doc in retrieved_documents]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    ranks = np.argsort(scores)[::-1]\n",
    "    ranked_docs = {rank_num: {text_field: retrieved_documents[rank_num][text_field], **retrieved_documents[rank_num]} for rank_num in ranks}\n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-ranking\n",
    "text_field = 'context'\n",
    "ranked_context = rank_documents(cross_encoder, text_field, query, context_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort\n",
    "sorted_ranked_context = dict(sorted(ranked_context.items())[:top_reranked_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format\n",
    "sorted_ranked_context = list(sorted_ranked_context.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'context': 'De la acusación y denuncia falsas y de la\\nsimulación de delitos\\n\\nArtículo 456\\n1. Los que, con conocimiento de su falsedad o temerario desprecio hacia la verdad, imputaren a alguna persona hechos\\nque, de ser ciertos, constituirían infracción penal, si esta\\n295',\n",
       "  'score': 0.664225459},\n",
       " {'context': 'Trata de seres humanos, 177 bis.\\nTratos degradantes, 173 a 177.\\nTregua:\\nViolación, 593.\\nTribunal Constitucional:\\nAtentado contra sus miembros, 550.\\nCalumnias, injurias o amenazas contra, 504.\\nTribunal de Cuentas:\\nObstáculos a la investigación, 502.2.\\nTribunal Superior de Justicia:\\nCalumnias, injurias o amenazas contra, 504.\\nTribunal Supremo:\\nCalumnias, injurias o amenazas contra, 504.\\nTribunales:\\nPerturbación del orden, 558.\\nTutela, 120, 192 y 440.\\nInhabilitación para el ejercicio de, 46.\\n\\nU\\nUltrajes:\\nA España, Comunidades Autónomas, símbolos o emblemas, 543.\\nUrbanismo:\\nDelitos contra, 319 y 320.\\nUsurpación:\\nAlteración de términos o lindes, 246.\\nDistracción del curso de aguas, 247.\\nOcupación de inmuebles, 245.\\nUsurpación de atribuciones, 506 a 509.\\nUsurpación del estado civil, 401.\\n429\\n\\n\\x0cUsurpación de funciones públicas, 402.',\n",
       "  'score': 0.644809961},\n",
       " {'context': 'JUAN CARLOS I\\nRey de España\\nA todos los que la presente vieren y entendieren.\\nSabed: Que las Cortes Generales han aprobado y Yo vengo en\\nsancionar la siguiente Ley Orgánica:\\nEXPOSICIÓN DE MOTIVOS\\nSi se ha llegado a definir el ordenamiento jurídico como conjunto de normas que regulan el uso de la fuerza, puede entenderse fácilmente la importancia del Código Penal en cualquier\\nsociedad civilizada. El Código Penal define los delitos y faltas\\nque constituyen los presupuestos de la aplicación de la forma\\nsuprema que puede revestir el poder coactivo del Estado: la\\npena criminal. En consecuencia, ocupa un lugar preeminente en\\nel conjunto del ordenamiento, hasta el punto de que, no sin\\nrazón, se ha considerado como una especie de «Constitución\\nnegativa». El Código Penal ha de tutelar los valores y principios\\nbásicos de la convivencia social. Cuando esos valores y principios cambian, debe también cambiar. En nuestro país, sin\\nembargo, pese a las profundas modificaciones de orden social,',\n",
       "  'score': 0.642645717}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show\n",
    "sorted_ranked_context[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get max docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5120"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model ID\n",
    "max_model_tokens = config[\"max_model_tokens\"]\n",
    "\n",
    "# Show\n",
    "max_model_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(string: str, encoding_name: str = \"cl100k_base\") -> int:\n",
    "    # Get encoding from tiktoken\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    # Encode the string using the specified encoding\n",
    "    encoded_string = encoding.encode(string)\n",
    "    # Count the number of tokens\n",
    "    num_tokens = len(encoded_string)\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize cumulative token count\n",
    "cumulative_tokens = 0\n",
    "\n",
    "# Filtered list to store dictionaries\n",
    "filtered_context = []\n",
    "\n",
    "# Iterate through the list of dictionaries\n",
    "for item in sorted_ranked_context:\n",
    "    # Calculate number of tokens for 'context' value\n",
    "    token_count = count_tokens(item['context'])\n",
    "    \n",
    "    # Cumulative sum of token counts\n",
    "    cumulative_tokens += token_count\n",
    "    \n",
    "    # Check if cumulative tokens are still less than max_model_tokens\n",
    "    if cumulative_tokens < max_model_tokens:\n",
    "        filtered_context.append(item)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'context': 'De la acusación y denuncia falsas y de la\\nsimulación de delitos\\n\\nArtículo 456\\n1. Los que, con conocimiento de su falsedad o temerario desprecio hacia la verdad, imputaren a alguna persona hechos\\nque, de ser ciertos, constituirían infracción penal, si esta\\n295',\n",
       "  'score': 0.664225459},\n",
       " {'context': 'Trata de seres humanos, 177 bis.\\nTratos degradantes, 173 a 177.\\nTregua:\\nViolación, 593.\\nTribunal Constitucional:\\nAtentado contra sus miembros, 550.\\nCalumnias, injurias o amenazas contra, 504.\\nTribunal de Cuentas:\\nObstáculos a la investigación, 502.2.\\nTribunal Superior de Justicia:\\nCalumnias, injurias o amenazas contra, 504.\\nTribunal Supremo:\\nCalumnias, injurias o amenazas contra, 504.\\nTribunales:\\nPerturbación del orden, 558.\\nTutela, 120, 192 y 440.\\nInhabilitación para el ejercicio de, 46.\\n\\nU\\nUltrajes:\\nA España, Comunidades Autónomas, símbolos o emblemas, 543.\\nUrbanismo:\\nDelitos contra, 319 y 320.\\nUsurpación:\\nAlteración de términos o lindes, 246.\\nDistracción del curso de aguas, 247.\\nOcupación de inmuebles, 245.\\nUsurpación de atribuciones, 506 a 509.\\nUsurpación del estado civil, 401.\\n429\\n\\n\\x0cUsurpación de funciones públicas, 402.',\n",
       "  'score': 0.644809961},\n",
       " {'context': 'JUAN CARLOS I\\nRey de España\\nA todos los que la presente vieren y entendieren.\\nSabed: Que las Cortes Generales han aprobado y Yo vengo en\\nsancionar la siguiente Ley Orgánica:\\nEXPOSICIÓN DE MOTIVOS\\nSi se ha llegado a definir el ordenamiento jurídico como conjunto de normas que regulan el uso de la fuerza, puede entenderse fácilmente la importancia del Código Penal en cualquier\\nsociedad civilizada. El Código Penal define los delitos y faltas\\nque constituyen los presupuestos de la aplicación de la forma\\nsuprema que puede revestir el poder coactivo del Estado: la\\npena criminal. En consecuencia, ocupa un lugar preeminente en\\nel conjunto del ordenamiento, hasta el punto de que, no sin\\nrazón, se ha considerado como una especie de «Constitución\\nnegativa». El Código Penal ha de tutelar los valores y principios\\nbásicos de la convivencia social. Cuando esos valores y principios cambian, debe también cambiar. En nuestro país, sin\\nembargo, pese a las profundas modificaciones de orden social,',\n",
       "  'score': 0.642645717}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show\n",
    "filtered_context[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens for all contexts in filtered_context: 3509\n"
     ]
    }
   ],
   "source": [
    "# Sum tokens for all contents in filtered_data\n",
    "total_tokens = sum(count_tokens(item[\"context\"]) for item in filtered_context)\n",
    "\n",
    "# Print total tokens\n",
    "print(\"Total tokens for all contexts in filtered_context:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the RAG model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter contexts, keeping only the context strings\n",
    "filtered_context_ready = [item[\"context\"] for item in filtered_context]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced model\n",
    "enhanced_model = llm_chain({\"context\": str(filtered_context_ready), \"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "enhanced_result = enhanced_model['text'].strip()\n",
    "\n",
    "# Markdown\n",
    "display(Markdown(f\"<b>{query}</b>\"))\n",
    "display(Markdown(f\"<p>{enhanced_result}</p>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA information\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    allocated_memory = torch.cuda.memory_allocated(0) / (1024**3)  # Convert bytes to GB\n",
    "    cached_memory = torch.cuda.memory_reserved(0) / (1024**3)  # Convert bytes to GB\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Convert bytes to GB\n",
    "    available_memory = total_memory - cached_memory\n",
    "    print('Allocated:   ', round(allocated_memory, 1), 'GB')\n",
    "    print('Cached:      ', round(cached_memory, 1), 'GB')\n",
    "    print('Available:  ', round(available_memory, 1), 'GB')\n",
    "    print('Total:      ', round(total_memory, 1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
