{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python version\n",
    "import sys \n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Variables\n",
    "from dotenv import load_dotenv\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "# Load env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch config\n",
    "from torch import cuda, bfloat16, float16\n",
    "import torch\n",
    "\n",
    "# Torch options\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters from YAML file\n",
    "import os\n",
    "\n",
    "# Change the current working directory to the directory containing the YAML file\n",
    "os.chdir('C:/Users/polri/Desktop/Git_TFM/TFM_LAW_LLM')\n",
    "\n",
    "# Load parameters from YAML file\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use optimum\n",
    "use_optimum = config[\"use_optimum\"]\n",
    "\n",
    "# Show\n",
    "use_optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory to file location\n",
    "from pathlib import Path\n",
    "import sys\n",
    "notebook_location = Path(os.path.abspath(''))\n",
    "os.chdir(notebook_location)\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "current_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "# CUDA information\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    allocated_memory = torch.cuda.memory_allocated(0) / (1024**3)  # Convert bytes to GB\n",
    "    cached_memory = torch.cuda.memory_reserved(0) / (1024**3)  # Convert bytes to GB\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Convert bytes to GB\n",
    "    available_memory = total_memory - cached_memory\n",
    "    print('Allocated:   ', round(allocated_memory, 1), 'GB')\n",
    "    print('Cached:      ', round(cached_memory, 1), 'GB')\n",
    "    print('Available:  ', round(available_memory, 1), 'GB')\n",
    "    print('Total:      ', round(total_memory, 1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init pinecone\n",
    "pinecone = Pinecone(api_key = \"03b29f67-c297-4462-825b-13ce23b3d577\")\n",
    "\n",
    "pc = Pinecone(api_key = pinecone)\n",
    "# Connect\n",
    "index_name = 'lawllm-unstructured-database'\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "# Index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model ID\n",
    "embed_model_id = config[\"embedding_model\"]\n",
    "\n",
    "# Show\n",
    "embed_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed model\n",
    "embed_model = HuggingFaceEmbeddings(\n",
    "    model_name = embed_model_id,\n",
    "    model_kwargs = {'device': device},\n",
    "    encode_kwargs = {'device': device, 'batch_size': 32}\n",
    ") \n",
    "\n",
    "# Show\n",
    "embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA information\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    allocated_memory = torch.cuda.memory_allocated(0) / (1024**3)  # Convert bytes to GB\n",
    "    cached_memory = torch.cuda.memory_reserved(0) / (1024**3)  # Convert bytes to GB\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Convert bytes to GB\n",
    "    available_memory = total_memory - cached_memory\n",
    "    print('Allocated:   ', round(allocated_memory, 1), 'GB')\n",
    "    print('Cached:      ', round(cached_memory, 1), 'GB')\n",
    "    print('Available:  ', round(available_memory, 1), 'GB')\n",
    "    print('Total:      ', round(total_memory, 1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load LLM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model ID\n",
    "use_quantization = config[\"use_quantization\"]\n",
    "\n",
    "# Show\n",
    "use_quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model\n",
    "model_id = config[\"model\"]\n",
    "\n",
    "# Show\n",
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set BNB configuration if quantization is enabled\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ") if use_quantization else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code = True,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map = \"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA information\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    allocated_memory = torch.cuda.memory_allocated(0) / (1024**3)  # Convert bytes to GB\n",
    "    cached_memory = torch.cuda.memory_reserved(0) / (1024**3)  # Convert bytes to GB\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Convert bytes to GB\n",
    "    available_memory = total_memory - cached_memory\n",
    "    print('Allocated:   ', round(allocated_memory, 1), 'GB')\n",
    "    print('Cached:      ', round(cached_memory, 1), 'GB')\n",
    "    print('Available:  ', round(available_memory, 1), 'GB')\n",
    "    print('Total:      ', round(total_memory, 1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pre-prompt\n",
    "pre_prompt = config[\"pre_prompt\"]\n",
    "\n",
    "# Create prompt context\n",
    "prompt_context = config[\"prompt_context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General template\n",
    "general_template = pre_prompt + prompt_context + \"A continuaci√≥n se proporciona el contexto: {context}\" + \" \" + \"pregunta: {query}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mistral template\n",
    "mistral_template = \"<s>[INST]\" + pre_prompt + prompt_context +  \"A continuaci√≥n se proporciona el contexto: [/INST] {context}\" + \"</s>\" + \"[INST] pregunta: {query} [/INST]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google template\n",
    "google_template = f\"\"\"\n",
    "<start_of_turn>user\n",
    "{pre_prompt}. {prompt_context} A continuaci√≥n se proporciona el contexto: \n",
    "Contexto: {{context}} \n",
    "Pregunta: {{query}}\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\n",
    "Respuesta: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the final template selection logic\n",
    "if \"mistral\" in model_id:\n",
    "    template = mistral_template\n",
    "    selected_template_message = \"Mistral template selected.\"\n",
    "elif \"google\" in model_id:\n",
    "    template = google_template\n",
    "    selected_template_message = \"Google template selected.\"\n",
    "else:\n",
    "    template = general_template\n",
    "    selected_template_message = \"Default template selected.\"\n",
    "\n",
    "# Print out the selected template message\n",
    "print(selected_template_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt Template\n",
    "prompt = PromptTemplate(\n",
    "    template = template, \n",
    "    input_variables = [\"context\", \"query\"]\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline with parameters from config file\n",
    "generate_text = transformers.pipeline(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    task = 'text-generation',\n",
    "    return_full_text = config[\"return_full_text\"],\n",
    "    max_new_tokens = config[\"max_new_tokens\"],\n",
    "    repetition_penalty = config[\"repetition_penalty\"],\n",
    "    temperature = config[\"temperature\"],\n",
    "    pad_token_id = tokenizer.eos_token_id,\n",
    "    batch_size = 1\n",
    ")\n",
    "\n",
    "# HF pipeline\n",
    "llm = HuggingFacePipeline(pipeline = generate_text)\n",
    "\n",
    "# Create llm chain \n",
    "llm_chain = LLMChain(llm = llm, prompt = prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text field\n",
    "text_field = \"text\"  \n",
    "\n",
    "# Vector store\n",
    "vectorstore = PineconeVectorStore(index, embed_model, text_field)  \n",
    "\n",
    "# Show\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple context\n",
    "context = \"Eres una API con conocimientos legales. Debes responder a preguntas en Espa√±ol. Si no conoces la respuesta, adm√≠telo.\"\n",
    "\n",
    "# Query\n",
    "query = 'Expl√≠came el Art√≠culo 245 del C√≥digo Penal de Espa√±a referente a ocupaciones ilegales de bienes inmuebles'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find closer Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity output\n",
    "similarity_output = vectorstore.similarity_search_with_score(query, k = config['top_k_docs'])\n",
    "\n",
    "# Context preprocessed\n",
    "context_processed = [{\"context\": doc.page_content, \"score\": score} for doc, score in similarity_output]\n",
    "\n",
    "# Show\n",
    "context_processed[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model ID\n",
    "reranking_model = config[\"reranking_model\"]\n",
    "\n",
    "# Show\n",
    "reranking_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model ID\n",
    "top_reranked_docs = config[\"top_reranked_docs\"]\n",
    "\n",
    "# Show\n",
    "top_reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting 'title' keys\n",
    "final_context = [entry['context'] for entry in context_processed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross encoder\n",
    "cross_encoder = CrossEncoder(reranking_model)\n",
    "\n",
    "# Show\n",
    "cross_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA information\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    allocated_memory = torch.cuda.memory_allocated(0) / (1024**3)  # Convert bytes to GB\n",
    "    cached_memory = torch.cuda.memory_reserved(0) / (1024**3)  # Convert bytes to GB\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Convert bytes to GB\n",
    "    available_memory = total_memory - cached_memory\n",
    "    print('Allocated:   ', round(allocated_memory, 1), 'GB')\n",
    "    print('Cached:      ', round(cached_memory, 1), 'GB')\n",
    "    print('Available:  ', round(available_memory, 1), 'GB')\n",
    "    print('Total:      ', round(total_memory, 1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-ranking\n",
    "text_field = 'context'\n",
    "ranked_context = rank_documents(cross_encoder, text_field, query, context_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort\n",
    "sorted_ranked_context = dict(sorted(ranked_context.items())[:top_reranked_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format\n",
    "sorted_ranked_context = list(sorted_ranked_context.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show\n",
    "sorted_ranked_context[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare your Dataset\n",
    "# Define your dataset and data loaders here\n",
    "\n",
    "# 2. Fine-Tuning Configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=8,   # batch size per device during training\n",
    "    per_device_eval_batch_size=8,    # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "# 3. Load Pre-Trained Model\n",
    "model_name = \"bert-base-uncased\"  # Example pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "# 4. Define Fine-Tuning Loop\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ü§ó Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=eval_dataset            # evaluation dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# 5. Evaluation\n",
    "trainer.evaluate()\n",
    "\n",
    "# 6. Save Fine-Tuned Model\n",
    "model.save_pretrained(\"./fine_tuned_model\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
