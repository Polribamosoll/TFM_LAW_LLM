FROM nvcr.io/nvidia/tritonserver:23.10-trtllm-python-py3

LABEL maintainer="Morgan Funtowicz <morgan@hf.co>"

ARG VCS_REF
ARG BUILD_DATE
ARG BUILD_VERSION

LABEL org.label-schema.schema-version="1.0"
LABEL org.label-schema.name="huggingface/inference-endpoints-trtllm"
LABEL org.label-schema.build-date=$BUILD_DATE
LABEL org.label-schema.version=$BUILD_VERSION
LABEL org.label-schema.vcs-ref=$VCS_REF
LABEL org.label-schema.vendor="Hugging Face Inc."
LABEL org.label-schema.version="1.0.0"
LABEL org.label-schema.url="https://hf.co/hardware"
LABEL org.label-schema.vcs-url="https://github.com/huggingface/optimum-nvidia"
LABEL org.label-schema.decription="Hugging Face Inference Server docker image for TensorRT-LLM Inference"

ENV HF_HUB_TOKEN ""


# Expose (in-order) HTTP, GRPC, Metrics endpoints
EXPOSE 8000/tcp
EXPOSE 8001/tcp
EXPOSE 8002/tcp

WORKDIR /repository

#ENTRYPOINT "huggingface-cli login --token ${HF_HUB_TOKEN}
CMD ["mpirun", "--allow-run-as-root", "-n", "1", "/opt/tritonserver/bin/tritonserver", "--exit-on-error=false", "--model-repo=/repository"]