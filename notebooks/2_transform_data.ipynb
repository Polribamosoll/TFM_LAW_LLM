{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (6.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: torch in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.66.2)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.12.3)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.6.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\polri\\appdata\\roaming\\python\\python312\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading tiktoken-0.6.0-cp312-cp312-win_amd64.whl (798 kB)\n",
      "   ---------------------------------------- 0.0/798.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/798.3 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/798.3 kB ? eta -:--:--\n",
      "   --- ----------------------------------- 61.4/798.3 kB 656.4 kB/s eta 0:00:02\n",
      "   ------------- -------------------------- 276.5/798.3 kB 2.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 471.0/798.3 kB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  788.5/798.3 kB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 798.3/798.3 kB 3.9 MB/s eta 0:00:00\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyyaml pandas torch tqdm python-dotenv beautifulsoup4 tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# Python version\n",
    "import sys \n",
    "print(sys. version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Environment Variables\n",
    "from dotenv import load_dotenv\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "# # Load env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the needed parameters from a yaml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters from YAML file\n",
    "import os\n",
    "\n",
    "# Change the current working directory to the directory containing the YAML file\n",
    "os.chdir('C:/Users/polri/Desktop/Git_TFM/TFM_LAW_LLM')\n",
    "\n",
    "import yaml\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@ayhamboucher/llm-based-context-splitter-for-large-documents-445d3f02b01b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.pinecone.io/learn/chunking-strategies/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\polri\\\\Desktop\\\\Git_TFM\\\\TFM_LAW_LLM'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set directory to file location\n",
    "from pathlib import Path\n",
    "import sys\n",
    "notebook_location = Path(os.path.abspath(''))\n",
    "os.chdir(notebook_location)\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "current_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries\n",
    "\n",
    "# Time data managment libraries \n",
    "import time\n",
    "\n",
    "# Garbage Collecting library: automatically freeing up memory occupied by objects that are no longer in use by the program\n",
    "import gc\n",
    "\n",
    "# Local assets: Utils contains functions we will later use\n",
    "from utils import *\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Deep learning tasks\n",
    "import torch\n",
    "\n",
    "# Progress bars\n",
    "import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Directory managment\n",
    "import os\n",
    "\n",
    "# Splitters\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing the notebook run\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "# CUDA information\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    allocated_memory = torch.cuda.memory_allocated(0) / (1024**3)  # Convert bytes to GB\n",
    "    cached_memory = torch.cuda.memory_reserved(0) / (1024**3)  # Convert bytes to GB\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Convert bytes to GB\n",
    "    available_memory = total_memory - cached_memory\n",
    "    print('Allocated:   ', round(allocated_memory, 1), 'GB')\n",
    "    print('Cached:      ', round(cached_memory, 1), 'GB')\n",
    "    print('Available:  ', round(available_memory, 1), 'GB')\n",
    "    print('Total:      ', round(total_memory, 1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1798"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data that we extracted and saved in the 1_extract_data.ipynb notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Params\n",
    "local_folder_path = \"C:/Users/polri/Desktop/Git_TFM/TFM_LAW_LLM/raw_data/txt_files\"\n",
    "file_list = [\"boe.txt\"]\n",
    "\n",
    "# List CSV files locally to see what we have\n",
    "local_csv_files = list_csv_files(local_folder_path)\n",
    "local_csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded boe.txt from local storage.\n",
      "                                                text\n",
      "0  LEY ORGÁNICA 10/1995, DE 23 DE NOVIEMBRE, DEL\\...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to read a text file\n",
    "def read_txt_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "    return data\n",
    "\n",
    "# Params\n",
    "local_folder_path = \"C:/Users/polri/Desktop/Git_TFM/TFM_LAW_LLM/raw_data/txt_files\"\n",
    "file_list = [\"boe.txt\"]\n",
    "\n",
    "# List to store text data\n",
    "text_data = []\n",
    "\n",
    "# Loop through files\n",
    "for file_name in file_list:\n",
    "    try:\n",
    "        # Load the text from the file\n",
    "        local_file_path = os.path.join(local_folder_path, file_name)\n",
    "        text = read_txt_file(local_file_path)\n",
    "        print(f\"Loaded {file_name} from local storage.\")\n",
    "        \n",
    "        # Append text to the list\n",
    "        text_data.append({'text': text})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "# Create DataFrame from the list of text data\n",
    "df_txt = pd.DataFrame(text_data)\n",
    "\n",
    "# Show DataFrame\n",
    "print(df_txt.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing boe.txt: Error tokenizing data. C error: Expected 3 fields in line 27, saw 4\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Check for each file in file_list\n",
    "# df_txt = pd.DataFrame()\n",
    "\n",
    "# # Loop by files\n",
    "# for file_name in file_list:\n",
    "#     try:\n",
    "#         # Load the file from local storage\n",
    "#         local_file_path = os.path.join(local_folder_path, file_name)\n",
    "#         df = pd.read_csv(local_file_path)\n",
    "#         df['id'] = df['id'].astype(str)\n",
    "#         print(f\"Loaded {file_name} from local storage.\")\n",
    "\n",
    "#         # Concatenate or process the DataFrame as needed\n",
    "#         df_txt = pd.concat([df_txt, df], ignore_index=True)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "# # Show\n",
    "# df_txt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the file: 731024\n"
     ]
    }
   ],
   "source": [
    "# Open the .txt file with the correct encoding\n",
    "with open('C:/Users/polri/Desktop/Git_TFM/TFM_LAW_LLM/raw_data/txt_files/boe.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the entire content of the file\n",
    "    df_txt = file.read()\n",
    "\n",
    "# Calculate the length of the file content\n",
    "file_length = len(df_txt)\n",
    "\n",
    "# Print the length of the file content\n",
    "print(\"Length of the file:\", file_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Filter empty texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove empty texts, seeking efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Calculate the character count for each row in the specified column\n",
    "# df_txt['character_count'] = df_txt['text'].apply(lambda x: len(str(x)) if pd.notna(x) else 0)\n",
    "\n",
    "# # Filter rows based on the character count condition\n",
    "# df_txt = df_txt[df_txt['character_count'] > 0].copy()\n",
    "\n",
    "# # Drop the temporary 'character_count' column\n",
    "# df_txt = df_txt.drop(columns=['character_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "388"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Length of file\n",
    "# len(df_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the maximum number of tokens a chunk can have and the chunk overlap size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum length of a text\n",
    "max_chunk_size = config['max_chunk_size']\n",
    "\n",
    "# Chunk overlap\n",
    "chunk_overlap_size = config['chunk_overlap']\n",
    "\n",
    "# Separators\n",
    "separators = [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use recursive splitter, as it has been proved to be the best performing splitter for problems like our."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = max_chunk_size,\n",
    "    chunk_overlap  = chunk_overlap_size,\n",
    "    length_function = len,\n",
    "    separators = separators\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliar function\n",
    "def recursive_text_splitter(text):\n",
    "    chunks = splitter.split_text(text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split BOE in smaller chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting Chunks: 100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chunk_0_1</td>\n",
       "      <td>LEY ORGÁNICA 10/1995, DE 23 DE NOVIEMBRE, DEL\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chunk_0_2</td>\n",
       "      <td>JUAN CARLOS I\\nRey de España\\nA todos los que ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chunk_0_3</td>\n",
       "      <td>negativa». El Código Penal ha de tutelar los v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chunk_0_4</td>\n",
       "      <td>sucinto, los criterios en que se inspira, aunq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chunk_0_5</td>\n",
       "      <td>menos básicos, y, de otra, introduce cambios e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     text_id                                               text\n",
       "0  chunk_0_1  LEY ORGÁNICA 10/1995, DE 23 DE NOVIEMBRE, DEL\\...\n",
       "1  chunk_0_2  JUAN CARLOS I\\nRey de España\\nA todos los que ...\n",
       "2  chunk_0_3  negativa». El Código Penal ha de tutelar los v...\n",
       "3  chunk_0_4  sucinto, los criterios en que se inspira, aunq...\n",
       "4  chunk_0_5  menos básicos, y, de otra, introduce cambios e..."
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split each text in the 'text' column into chunks recursively\n",
    "df_txt['chunks'] = df_txt['text'].apply(lambda x: recursive_text_splitter(x))\n",
    "\n",
    "# Create a new DataFrame with individual chunks and unique identifiers\n",
    "chunks_list = []\n",
    "for idx, row in tqdm(df_txt.iterrows(), total=len(df_txt), desc=\"Splitting Chunks\"):\n",
    "    for i, chunk in enumerate(row['chunks']):\n",
    "        chunks_list.append({\n",
    "            'text_id': f\"chunk_{idx}_{i+1}\",  # Generate unique identifier based on file index and chunk index\n",
    "            'text': chunk\n",
    "        })\n",
    "\n",
    "# Create a new DataFrame for the chunks\n",
    "splitted_df_v1 = pd.DataFrame(chunks_list)\n",
    "\n",
    "# Show\n",
    "splitted_df_v1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1068"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Length of file/\n",
    "len(splitted_df_v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now save the already splitted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save splitted data\n",
    "path = 'prepared_data/'\n",
    "csv_file_name_v1 = f'{path}splitted_input_base.csv'\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "splitted_df_v1.to_csv(csv_file_name_v1, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we can see the different chunks of the BOE, among it's format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3837"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the runtime from the recursive splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in the 'text' column: 238500\n"
     ]
    }
   ],
   "source": [
    "# Sum total tokens\n",
    "total_tokens_processed = splitted_df_v1['text'].apply(lambda x: count_tokens(x)).sum()\n",
    "\n",
    "# Show\n",
    "print(\"Total number of tokens in the 'text' column:\", total_tokens_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0 hours and 24 minutes.\n"
     ]
    }
   ],
   "source": [
    "# End time of notebook run\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Convert elapsed time to hours and minutes\n",
    "hours = int(elapsed_time // 3600)\n",
    "minutes = int((elapsed_time % 3600) // 60)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Time elapsed: {hours} hours and {minutes} minutes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
