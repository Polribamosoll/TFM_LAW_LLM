{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyyaml in c:\\users\\i0550306\\appdata\\roaming\\python\\python310\\site-packages (6.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\i0550306\\appdata\\roaming\\python\\python310\\site-packages (2.2.2)\n",
      "Requirement already satisfied: torch in c:\\users\\i0550306\\appdata\\roaming\\python\\python310\\site-packages (2.2.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\i0550306\\appdata\\roaming\\python\\python310\\site-packages (4.66.2)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\i0550306\\appdata\\roaming\\python\\python310\\site-packages (1.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\i0550306\\appdata\\roaming\\python\\python310\\site-packages (4.12.3)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\i0550306\\appdata\\roaming\\python\\python310\\site-packages (0.6.0)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.1.16-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\i0550306\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\i0550306\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\i0550306\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\i0550306\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\i0550306\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\i0550306\\appdata\\roaming\\python\\python310\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\i0550306\\appdata\\roaming\\python\\python310\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\i0550306\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\i0550306\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\i0550306\\appdata\\roaming\\python\\python310\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\i0550306\\appdata\\roaming\\python\\python310\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\i0550306\\appdata\\roaming\\python\\python310\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\i0550306\\appdata\\roaming\\python\\python310\\site-packages (from tiktoken) (2024.4.16)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\i0550306\\appdata\\roaming\\python\\python310\\site-packages (from tiktoken) (2.31.0)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading SQLAlchemy-2.0.29-cp310-cp310-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Downloading aiohttp-3.9.5-cp310-cp310-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
      "  Downloading dataclasses_json-0.6.4-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.32 (from langchain)\n",
      "  Downloading langchain_community-0.0.34-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting langchain-core<0.2.0,>=0.1.42 (from langchain)\n",
      "  Downloading langchain_core-0.1.45-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.49-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydantic<3,>=1 (from langchain)\n",
      "  Downloading pydantic-2.7.0-py3-none-any.whl.metadata (103 kB)\n",
      "     ---------------------------------------- 0.0/103.4 kB ? eta -:--:--\n",
      "     -------------------------------------- 103.4/103.4 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain)\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading multidict-6.0.5-cp310-cp310-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-win_amd64.whl.metadata (32 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading marshmallow-3.21.1-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
      "  Downloading jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.42->langchain)\n",
      "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.1-cp310-none-win_amd64.whl.metadata (50 kB)\n",
      "     ---------------------------------------- 0.0/50.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 50.9/50.9 kB 1.3 MB/s eta 0:00:00\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.18.1 (from pydantic<3,>=1->langchain)\n",
      "  Downloading pydantic_core-2.18.1-cp310-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\i0550306\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\i0550306\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\i0550306\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\i0550306\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\i0550306\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.0.3-cp310-cp310-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\i0550306\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\i0550306\\appdata\\roaming\\python\\python310\\site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading langchain-0.1.16-py3-none-any.whl (817 kB)\n",
      "   ---------------------------------------- 0.0/817.7 kB ? eta -:--:--\n",
      "   --------------------------------------  809.0/817.7 kB 25.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 817.7/817.7 kB 8.6 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.9.5-cp310-cp310-win_amd64.whl (370 kB)\n",
      "   ---------------------------------------- 0.0/370.7 kB ? eta -:--:--\n",
      "   --------------------------------------  368.6/370.7 kB 22.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 370.7/370.7 kB 7.7 MB/s eta 0:00:00\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_community-0.0.34-py3-none-any.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ------------------------------------- -- 1.8/1.9 MB 58.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 40.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 20.5 MB/s eta 0:00:00\n",
      "Downloading langchain_core-0.1.45-py3-none-any.whl (291 kB)\n",
      "   ---------------------------------------- 0.0/291.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 291.3/291.3 kB 9.1 MB/s eta 0:00:00\n",
      "Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Downloading langsmith-0.1.49-py3-none-any.whl (115 kB)\n",
      "   ---------------------------------------- 0.0/115.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 115.2/115.2 kB 3.3 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.7.0-py3-none-any.whl (407 kB)\n",
      "   ---------------------------------------- 0.0/407.9 kB ? eta -:--:--\n",
      "   --------------------------------------  399.4/407.9 kB 24.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 407.9/407.9 kB 8.5 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.18.1-cp310-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ------------------------------------- -- 1.8/1.9 MB 55.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 30.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 17.3 MB/s eta 0:00:00\n",
      "Downloading SQLAlchemy-2.0.29-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 1.9/2.1 MB 41.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.1/2.1 MB 44.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 22.2 MB/s eta 0:00:00\n",
      "Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "   ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 60.8/60.8 kB 3.2 MB/s eta 0:00:00\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-win_amd64.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.4/50.4 kB 855.2 kB/s eta 0:00:00\n",
      "Downloading greenlet-3.0.3-cp310-cp310-win_amd64.whl (292 kB)\n",
      "   ---------------------------------------- 0.0/292.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 292.3/292.3 kB 9.1 MB/s eta 0:00:00\n",
      "Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
      "   ---------------------------------------- 0.0/49.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 49.4/49.4 kB 2.4 MB/s eta 0:00:00\n",
      "Downloading multidict-6.0.5-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Downloading orjson-3.10.1-cp310-none-win_amd64.whl (139 kB)\n",
      "   ---------------------------------------- 0.0/139.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 139.1/139.1 kB 4.2 MB/s eta 0:00:00\n",
      "Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "   ---------------------------------------- 0.0/53.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 53.0/53.0 kB 1.4 MB/s eta 0:00:00\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading yarl-1.9.4-cp310-cp310-win_amd64.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 76.4/76.4 kB 4.1 MB/s eta 0:00:00\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: tenacity, pydantic-core, packaging, orjson, mypy-extensions, multidict, jsonpointer, greenlet, frozenlist, attrs, async-timeout, annotated-types, yarl, typing-inspect, SQLAlchemy, pydantic, marshmallow, jsonpatch, aiosignal, langsmith, dataclasses-json, aiohttp, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.0\n",
      "    Uninstalling packaging-24.0:\n",
      "      Successfully uninstalled packaging-24.0\n",
      "Successfully installed SQLAlchemy-2.0.29 aiohttp-3.9.5 aiosignal-1.3.1 annotated-types-0.6.0 async-timeout-4.0.3 attrs-23.2.0 dataclasses-json-0.6.4 frozenlist-1.4.1 greenlet-3.0.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.16 langchain-community-0.0.34 langchain-core-0.1.45 langchain-text-splitters-0.0.1 langsmith-0.1.49 marshmallow-3.21.1 multidict-6.0.5 mypy-extensions-1.0.0 orjson-3.10.1 packaging-23.2 pydantic-2.7.0 pydantic-core-2.18.1 tenacity-8.2.3 typing-inspect-0.9.0 yarl-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pyyaml pandas torch tqdm python-dotenv beautifulsoup4 tiktoken langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import HfApi\n",
    "\n",
    "# def login(username, password):\n",
    "#     api = HfApi()\n",
    "#     api.login(username, password)\n",
    "\n",
    "# # Replace 'your_username' and 'your_password' with your actual Hugging Face username and password\n",
    "# username = 'your_username'\n",
    "# password = 'your_password'\n",
    "\n",
    "# login(username, password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# Python version\n",
    "import sys \n",
    "print(sys. version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Environment Variables\n",
    "from dotenv import load_dotenv\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "# # Load env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the needed parameters from a yaml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters from YAML file\n",
    "import os\n",
    "\n",
    "# Change the current working directory to the directory containing the YAML file\n",
    "os.chdir('C:/TFM/TFM_LAW_LLM')\n",
    "\n",
    "import yaml\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@ayhamboucher/llm-based-context-splitter-for-large-documents-445d3f02b01b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.pinecone.io/learn/chunking-strategies/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\TFM\\\\TFM_LAW_LLM'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set directory to file location\n",
    "from pathlib import Path\n",
    "import sys\n",
    "notebook_location = Path(os.path.abspath(''))\n",
    "os.chdir(notebook_location)\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "current_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries\n",
    "\n",
    "# Time data managment libraries \n",
    "import time\n",
    "\n",
    "# Garbage Collecting library: automatically freeing up memory occupied by objects that are no longer in use by the program\n",
    "import gc\n",
    "\n",
    "# Local assets: Utils contains functions we will later use\n",
    "from utils import *\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Deep learning tasks\n",
    "import torch\n",
    "\n",
    "# Progress bars\n",
    "import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Directory managment\n",
    "import os\n",
    "\n",
    "# Splitters\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing the notebook run\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "# CUDA information\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    allocated_memory = torch.cuda.memory_allocated(0) / (1024**3)  # Convert bytes to GB\n",
    "    cached_memory = torch.cuda.memory_reserved(0) / (1024**3)  # Convert bytes to GB\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Convert bytes to GB\n",
    "    available_memory = total_memory - cached_memory\n",
    "    print('Allocated:   ', round(allocated_memory, 1), 'GB')\n",
    "    print('Cached:      ', round(cached_memory, 1), 'GB')\n",
    "    print('Available:  ', round(available_memory, 1), 'GB')\n",
    "    print('Total:      ', round(total_memory, 1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data that we extracted and saved in the 1_extract_data.ipynb notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Params\n",
    "local_folder_path = \"C:/TFM/TFM_LAW_LLM/raw_data/txt_files\"\n",
    "file_list = [\"boe.txt\"]\n",
    "\n",
    "# List CSV files locally to see what we have\n",
    "local_csv_files = list_csv_files(local_folder_path)\n",
    "local_csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded boe.txt from local storage.\n",
      "                                                text\n",
      "0  LEY ORGÁNICA 10/1995, DE 23 DE NOVIEMBRE, DEL\\...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to read a text file\n",
    "def read_txt_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "    return data\n",
    "\n",
    "# Params\n",
    "local_folder_path = \"C:/TFM/TFM_LAW_LLM/raw_data/txt_files\"\n",
    "file_list = [\"boe.txt\"]\n",
    "\n",
    "# List to store text data\n",
    "text_data = []\n",
    "\n",
    "# Loop through files\n",
    "for file_name in file_list:\n",
    "    try:\n",
    "        # Load the text from the file\n",
    "        local_file_path = os.path.join(local_folder_path, file_name)\n",
    "        text = read_txt_file(local_file_path)\n",
    "        print(f\"Loaded {file_name} from local storage.\")\n",
    "        \n",
    "        # Append text to the list\n",
    "        text_data.append({'text': text})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "# Create DataFrame from the list of text data\n",
    "df_txt = pd.DataFrame(text_data)\n",
    "\n",
    "# Show DataFrame\n",
    "print(df_txt.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing boe.txt: Error tokenizing data. C error: Expected 3 fields in line 27, saw 4\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Check for each file in file_list\n",
    "# df_txt = pd.DataFrame()\n",
    "\n",
    "# # Loop by files\n",
    "# for file_name in file_list:\n",
    "#     try:\n",
    "#         # Load the file from local storage\n",
    "#         local_file_path = os.path.join(local_folder_path, file_name)\n",
    "#         df = pd.read_csv(local_file_path)\n",
    "#         df['id'] = df['id'].astype(str)\n",
    "#         print(f\"Loaded {file_name} from local storage.\")\n",
    "\n",
    "#         # Concatenate or process the DataFrame as needed\n",
    "#         df_txt = pd.concat([df_txt, df], ignore_index=True)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "# # Show\n",
    "# df_txt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the file: 731024\n"
     ]
    }
   ],
   "source": [
    "# Open the .txt file with the correct encoding\n",
    "with open('C:/TFM/TFM_LAW_LLM/raw_data/txt_files/boe.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the entire content of the file\n",
    "    df_txt = file.read()\n",
    "\n",
    "# Calculate the length of the file content\n",
    "file_length = len(df_txt)\n",
    "\n",
    "# Print the length of the file content\n",
    "print(\"Length of the file:\", file_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Filter empty texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove empty texts, seeking efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Calculate the character count for each row in the specified column\n",
    "# df_txt['character_count'] = df_txt['text'].apply(lambda x: len(str(x)) if pd.notna(x) else 0)\n",
    "\n",
    "# # Filter rows based on the character count condition\n",
    "# df_txt = df_txt[df_txt['character_count'] > 0].copy()\n",
    "\n",
    "# # Drop the temporary 'character_count' column\n",
    "# df_txt = df_txt.drop(columns=['character_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "388"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Length of file\n",
    "# len(df_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the maximum number of characters a chunk can have and the chunk overlap size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum length of a text\n",
    "max_chunk_size = config['max_chunk_size']\n",
    "\n",
    "# Chunk overlap\n",
    "chunk_overlap_size = config['chunk_overlap']\n",
    "\n",
    "# Separators\n",
    "separators = [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use recursive splitter, as it has been proved to be the best performing splitter for problems like our."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chunk_1</td>\n",
       "      <td>LEY ORGÁNICA 10/1995, DE 23 DE NOVIEMBRE, DEL\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chunk_2</td>\n",
       "      <td>JUAN CARLOS I\\nRey de España\\nA todos los que ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chunk_3</td>\n",
       "      <td>negativa». El Código Penal ha de tutelar los v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chunk_4</td>\n",
       "      <td>sucinto, los criterios en que se inspira, aunq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chunk_5</td>\n",
       "      <td>menos básicos, y, de otra, introduce cambios e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_id                                               text\n",
       "0  chunk_1  LEY ORGÁNICA 10/1995, DE 23 DE NOVIEMBRE, DEL\\...\n",
       "1  chunk_2  JUAN CARLOS I\\nRey de España\\nA todos los que ...\n",
       "2  chunk_3  negativa». El Código Penal ha de tutelar los v...\n",
       "3  chunk_4  sucinto, los criterios en que se inspira, aunq...\n",
       "4  chunk_5  menos básicos, y, de otra, introduce cambios e..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=max_chunk_size,\n",
    "    chunk_overlap=chunk_overlap_size,\n",
    "    length_function=len,\n",
    "    separators=separators\n",
    ")\n",
    "\n",
    "# Auxiliar function\n",
    "def recursive_text_splitter(text):\n",
    "    chunks = splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "# Split the text into chunks\n",
    "chunks = recursive_text_splitter(df_txt)\n",
    "\n",
    "# Create a list of dictionaries containing chunk data\n",
    "chunks_list = [{'text_id': f\"chunk_{i+1}\", 'text': chunk} for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# Create a DataFrame from the chunks list\n",
    "splitted_df_v1 = pd.DataFrame(chunks_list)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "splitted_df_v1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1068"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Length of file/\n",
    "len(splitted_df_v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now save the already splitted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save splitted data\n",
    "path = 'prepared_data/'\n",
    "csv_file_name_v1 = f'{path}splitted_input_base.csv'\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "splitted_df_v1.to_csv(csv_file_name_v1, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we can see the different chunks of the BOE, among it's format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2317"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the runtime from the recursive splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in the 'text' column: 238500\n"
     ]
    }
   ],
   "source": [
    "# Sum total tokens\n",
    "total_tokens_processed = splitted_df_v1['text'].apply(lambda x: count_tokens(x)).sum()\n",
    "\n",
    "# Show\n",
    "print(\"Total number of tokens in the 'text' column:\", total_tokens_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0 hours and 8 minutes.\n"
     ]
    }
   ],
   "source": [
    "# End time of notebook run\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Convert elapsed time to hours and minutes\n",
    "hours = int(elapsed_time // 3600)\n",
    "minutes = int((elapsed_time % 3600) // 60)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Time elapsed: {hours} hours and {minutes} minutes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
