{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (5.3.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (1.4.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.12.0+cu116)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (4.64.0)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (4.11.1)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.6.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting langchain\n",
      "  Downloading langchain-0.1.16-py3-none-any.whl (817 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from pandas) (1.23.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.9/dist-packages (from tiktoken) (2.28.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken) (2022.7.9)\n",
      "Collecting langchain-community<0.1,>=0.0.32\n",
      "  Downloading langchain_community-0.0.34-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.9.1)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Downloading aiohttp-3.9.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tenacity<9.0.0,>=8.1.0\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.9/dist-packages (from langchain) (1.4.39)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7\n",
      "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Collecting langchain-core<0.2.0,>=0.1.42\n",
      "  Downloading langchain_core-0.1.46-py3-none-any.whl (299 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.3/299.3 kB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from langchain) (4.0.2)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1\n",
      "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17\n",
      "  Downloading langsmith-0.1.51-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.0/116.0 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.7.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (18.2.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspect<1,>=0.4.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting jsonpointer>=1.9\n",
      "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Collecting packaging<24.0,>=23.2\n",
      "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14\n",
      "  Downloading orjson-3.10.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.9/140.9 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.14.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (1.26.10)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken) (2.1.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (1.1.2)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: tenacity, python-dotenv, packaging, orjson, mypy-extensions, jsonpointer, typing-inspect, tiktoken, marshmallow, langsmith, jsonpatch, aiohttp, langchain-core, dataclasses-json, langchain-text-splitters, langchain-community, langchain\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 21.3\n",
      "    Uninstalling packaging-21.3:\n",
      "      Successfully uninstalled packaging-21.3\n",
      "  Attempting uninstall: marshmallow\n",
      "    Found existing installation: marshmallow 2.21.0\n",
      "    Uninstalling marshmallow-2.21.0:\n",
      "      Successfully uninstalled marshmallow-2.21.0\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.8.1\n",
      "    Uninstalling aiohttp-3.8.1:\n",
      "      Successfully uninstalled aiohttp-3.8.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradient 2.0.5 requires marshmallow<3.0, but you have marshmallow 3.21.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiohttp-3.9.5 dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.16 langchain-community-0.0.34 langchain-core-0.1.46 langchain-text-splitters-0.0.1 langsmith-0.1.51 marshmallow-3.21.1 mypy-extensions-1.0.0 orjson-3.10.1 packaging-23.2 python-dotenv-1.0.1 tenacity-8.2.3 tiktoken-0.6.0 typing-inspect-0.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyyaml pandas torch tqdm python-dotenv beautifulsoup4 tiktoken langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import HfApi\n",
    "\n",
    "# def login(username, password):\n",
    "#     api = HfApi()\n",
    "#     api.login(username, password)\n",
    "\n",
    "# # Replace 'your_username' and 'your_password' with your actual Hugging Face username and password\n",
    "# username = 'your_username'\n",
    "# password = 'your_password'\n",
    "\n",
    "# login(username, password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.13 (main, May 23 2022, 22:01:06) \n",
      "[GCC 9.4.0]\n"
     ]
    }
   ],
   "source": [
    "# Python version\n",
    "import sys \n",
    "print(sys. version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Environment Variables\n",
    "from dotenv import load_dotenv\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "# # Load env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the needed parameters from a yaml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters from YAML file\n",
    "import os\n",
    "\n",
    "# Change the current working directory to the directory containing the YAML file\n",
    "os.chdir('/notebooks/TFM/TFM_LAW_LLM')\n",
    "\n",
    "import yaml\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@ayhamboucher/llm-based-context-splitter-for-large-documents-445d3f02b01b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.pinecone.io/learn/chunking-strategies/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/notebooks/TFM/TFM_LAW_LLM'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set directory to file location\n",
    "from pathlib import Path\n",
    "import sys\n",
    "notebook_location = Path(os.path.abspath(''))\n",
    "os.chdir(notebook_location)\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "current_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries\n",
    "\n",
    "# Time data managment libraries \n",
    "import time\n",
    "\n",
    "# Garbage Collecting library: automatically freeing up memory occupied by objects that are no longer in use by the program\n",
    "import gc\n",
    "\n",
    "# Local assets: Utils contains functions we will later use\n",
    "from utils import *\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Deep learning tasks\n",
    "import torch\n",
    "\n",
    "# Progress bars\n",
    "import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Directory managment\n",
    "import os\n",
    "\n",
    "# Splitters\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing the notebook run\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA RTX A4000\n",
      "Memory Usage:\n",
      "Allocated:    0.0 GB\n",
      "Cached:       0.0 GB\n",
      "Available:   15.7 GB\n",
      "Total:       15.7 GB\n"
     ]
    }
   ],
   "source": [
    "# Setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "# CUDA information\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    allocated_memory = torch.cuda.memory_allocated(0) / (1024**3)  # Convert bytes to GB\n",
    "    cached_memory = torch.cuda.memory_reserved(0) / (1024**3)  # Convert bytes to GB\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Convert bytes to GB\n",
    "    available_memory = total_memory - cached_memory\n",
    "    print('Allocated:   ', round(allocated_memory, 1), 'GB')\n",
    "    print('Cached:      ', round(cached_memory, 1), 'GB')\n",
    "    print('Available:  ', round(available_memory, 1), 'GB')\n",
    "    print('Total:      ', round(total_memory, 1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data that we extracted and saved in the 1_extract_data.ipynb notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Params\n",
    "local_folder_path = \"/notebooks/TFM/TFM_LAW_LLM/raw_data/txt_files\"\n",
    "file_list = [\"boe.txt\"]\n",
    "\n",
    "# List CSV files locally to see what we have\n",
    "local_csv_files = list_csv_files(local_folder_path)\n",
    "local_csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing boe.txt: [Errno 2] No such file or directory: 'C:/TFM/TFM_LAW_LLM/raw_data/txt_files/boe.txt'\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to read a text file\n",
    "def read_txt_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "    return data\n",
    "\n",
    "# Params\n",
    "local_folder_path = \"C:/TFM/TFM_LAW_LLM/raw_data/txt_files\"\n",
    "file_list = [\"boe.txt\"]\n",
    "\n",
    "# List to store text data\n",
    "text_data = []\n",
    "\n",
    "# Loop through files\n",
    "for file_name in file_list:\n",
    "    try:\n",
    "        # Load the text from the file\n",
    "        local_file_path = os.path.join(local_folder_path, file_name)\n",
    "        text = read_txt_file(local_file_path)\n",
    "        print(f\"Loaded {file_name} from local storage.\")\n",
    "        \n",
    "        # Append text to the list\n",
    "        text_data.append({'text': text})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "# Create DataFrame from the list of text data\n",
    "df_txt = pd.DataFrame(text_data)\n",
    "\n",
    "# Show DataFrame\n",
    "print(df_txt.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check for each file in file_list\n",
    "# df_txt = pd.DataFrame()\n",
    "\n",
    "# # Loop by files\n",
    "# for file_name in file_list:\n",
    "#     try:\n",
    "#         # Load the file from local storage\n",
    "#         local_file_path = os.path.join(local_folder_path, file_name)\n",
    "#         df = pd.read_csv(local_file_path)\n",
    "#         df['id'] = df['id'].astype(str)\n",
    "#         print(f\"Loaded {file_name} from local storage.\")\n",
    "\n",
    "#         # Concatenate or process the DataFrame as needed\n",
    "#         df_txt = pd.concat([df_txt, df], ignore_index=True)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "# # Show\n",
    "# df_txt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the file: 731024\n"
     ]
    }
   ],
   "source": [
    "# Open the .txt file with the correct encoding\n",
    "with open('/notebooks/TFM/TFM_LAW_LLM/raw_data/txt_files/boe.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the entire content of the file\n",
    "    df_txt = file.read()\n",
    "\n",
    "# Calculate the length of the file content\n",
    "file_length = len(df_txt)\n",
    "\n",
    "# Print the length of the file content\n",
    "print(\"Length of the file:\", file_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Filter empty texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove empty texts, seeking efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Calculate the character count for each row in the specified column\n",
    "# df_txt['character_count'] = df_txt['text'].apply(lambda x: len(str(x)) if pd.notna(x) else 0)\n",
    "\n",
    "# # Filter rows based on the character count condition\n",
    "# df_txt = df_txt[df_txt['character_count'] > 0].copy()\n",
    "\n",
    "# # Drop the temporary 'character_count' column\n",
    "# df_txt = df_txt.drop(columns=['character_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Length of file\n",
    "# len(df_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the maximum number of characters a chunk can have and the chunk overlap size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum length of a text\n",
    "max_chunk_size = config['max_chunk_size']\n",
    "\n",
    "# Chunk overlap\n",
    "chunk_overlap_size = config['chunk_overlap']\n",
    "\n",
    "# Separators\n",
    "separators = [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use recursive splitter, as it has been proved to be the best performing splitter for problems like our."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chunk_1</td>\n",
       "      <td>LEY ORGÁNICA 10/1995, DE 23 DE NOVIEMBRE, DEL\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chunk_2</td>\n",
       "      <td>JUAN CARLOS I\\nRey de España\\nA todos los que ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chunk_3</td>\n",
       "      <td>negativa». El Código Penal ha de tutelar los v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chunk_4</td>\n",
       "      <td>sucinto, los criterios en que se inspira, aunq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chunk_5</td>\n",
       "      <td>menos básicos, y, de otra, introduce cambios e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_id                                               text\n",
       "0  chunk_1  LEY ORGÁNICA 10/1995, DE 23 DE NOVIEMBRE, DEL\\...\n",
       "1  chunk_2  JUAN CARLOS I\\nRey de España\\nA todos los que ...\n",
       "2  chunk_3  negativa». El Código Penal ha de tutelar los v...\n",
       "3  chunk_4  sucinto, los criterios en que se inspira, aunq...\n",
       "4  chunk_5  menos básicos, y, de otra, introduce cambios e..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=max_chunk_size,\n",
    "    chunk_overlap=chunk_overlap_size,\n",
    "    length_function=len,\n",
    "    separators=separators\n",
    ")\n",
    "\n",
    "# Auxiliar function\n",
    "def recursive_text_splitter(text):\n",
    "    chunks = splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "# Split the text into chunks\n",
    "chunks = recursive_text_splitter(df_txt)\n",
    "\n",
    "# Create a list of dictionaries containing chunk data\n",
    "chunks_list = [{'text_id': f\"chunk_{i+1}\", 'text': chunk} for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# Create a DataFrame from the chunks list\n",
    "splitted_df_v1 = pd.DataFrame(chunks_list)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "splitted_df_v1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1068"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Length of file/\n",
    "len(splitted_df_v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now save the already splitted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save splitted data\n",
    "path = 'prepared_data/'\n",
    "csv_file_name_v1 = f'{path}splitted_input_base.csv'\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "splitted_df_v1.to_csv(csv_file_name_v1, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we can see the different chunks of the BOE, among it's format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the runtime from the recursive splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in the 'text' column: 238500\n"
     ]
    }
   ],
   "source": [
    "# Sum total tokens\n",
    "total_tokens_processed = splitted_df_v1['text'].apply(lambda x: count_tokens(x)).sum()\n",
    "\n",
    "# Show\n",
    "print(\"Total number of tokens in the 'text' column:\", total_tokens_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0 hours and 0 minutes.\n"
     ]
    }
   ],
   "source": [
    "# End time of notebook run\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Convert elapsed time to hours and minutes\n",
    "hours = int(elapsed_time // 3600)\n",
    "minutes = int((elapsed_time % 3600) // 60)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Time elapsed: {hours} hours and {minutes} minutes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
