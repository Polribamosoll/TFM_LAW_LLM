{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook reproduces the methodology described in 4.1.2. Data Transformation, explicitly in 4.1.2.1 Recursive Text Splitter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first install the libraries we will use during this step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (6.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: torch in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.66.2)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: langchain in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.1.16)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\polri\\appdata\\roaming\\python\\python312\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.32 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.0.32)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.42 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.1.42)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.1.45)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.7.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.42->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.18.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\polri\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyyaml pandas torch tqdm python-dotenv beautifulsoup4 tiktoken langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# Python version\n",
    "import sys \n",
    "print(sys. version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to use the stored environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Environment Variables\n",
    "from dotenv import load_dotenv\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "# # Load env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the needed parameters from a yaml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters from YAML file\n",
    "os.chdir('/notebooks/TFM_LAW_LLM')\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@ayhamboucher/llm-based-context-splitter-for-large-documents-445d3f02b01b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.pinecone.io/learn/chunking-strategies/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\polri\\\\Desktop\\\\Git_TFM\\\\TFM_LAW_LLM'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set directory \n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "notebook_location = Path(os.path.abspath(''))\n",
    "os.chdir(notebook_location)\n",
    "current_directory = os.getcwd()\n",
    "current_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries\n",
    "\n",
    "# Time data managment libraries \n",
    "import time\n",
    "\n",
    "# Garbage Collecting library: automatically freeing up memory occupied by objects that are no longer in use by the program\n",
    "import gc\n",
    "\n",
    "# Local assets: Utils contains functions we will later use\n",
    "from utils import *\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Deep learning tasks\n",
    "import torch\n",
    "\n",
    "# Progress bars\n",
    "import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Splitters\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing the notebook run\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's how we make sure we are using GPU if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "# CUDA information\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    allocated_memory = torch.cuda.memory_allocated(0) / (1024**3)  # Convert bytes to GB\n",
    "    cached_memory = torch.cuda.memory_reserved(0) / (1024**3)  # Convert bytes to GB\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Convert bytes to GB\n",
    "    available_memory = total_memory - cached_memory\n",
    "    print('Allocated:   ', round(allocated_memory, 1), 'GB')\n",
    "    print('Cached:      ', round(cached_memory, 1), 'GB')\n",
    "    print('Available:  ', round(available_memory, 1), 'GB')\n",
    "    print('Total:      ', round(total_memory, 1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clean the cache so we have space in cuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data that we extracted and saved in the 1_extract_data.ipynb notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Params\n",
    "local_folder_path = \"/notebooks/TFM_LAW_LLM/raw_data/txt_files\"\n",
    "file_list = [\"boe.txt\"]\n",
    "\n",
    "# List CSV files locally to see what we have, this was meant to be use in an other use case, not the one in this thesis.\n",
    "local_csv_files = list_csv_files(local_folder_path)\n",
    "local_csv_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now open the boe .txt file that was downloaded from the official site of Spanish Government, and convert it into a dataframe format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded boe.txt from local storage.\n",
      "                                                text\n",
      "0  LEY ORGÁNICA 10/1995, DE 23 DE NOVIEMBRE, DEL\\...\n"
     ]
    }
   ],
   "source": [
    "def read_txt_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "    return data\n",
    "\n",
    "# Params\n",
    "local_folder_path = \"/notebooks/TFM_LAW_LLM/raw_data/txt_files\"\n",
    "file_list = [\"boe.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will temporally store the data in this file\n",
    "text_data = []\n",
    "\n",
    "# Loop through any files that we may have\n",
    "for file_name in file_list:\n",
    "    try:\n",
    "        local_file_path = os.path.join(local_folder_path, file_name)\n",
    "        text = read_txt_file(local_file_path)\n",
    "        print(f\"Loaded {file_name} from local storage.\")\n",
    "        text_data.append({'text': text})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "\n",
    "df_txt = pd.DataFrame(text_data)\n",
    "print(df_txt.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the .txt length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the file: 731024\n"
     ]
    }
   ],
   "source": [
    "with open('/notebooks/TFM_LAW_LLM/raw_data/txt_files/boe.txt', 'r', encoding='utf-8') as file:\n",
    "    df_txt = file.read()\n",
    "\n",
    "file_length = len(df_txt)\n",
    "print(\"Length of the file:\", file_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the maximum number of characters a chunk can have and the chunk overlap size. These parameters are stored in the configuration yaml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum length of a text\n",
    "max_chunk_size = config['max_chunk_size']\n",
    "\n",
    "# Chunk overlap\n",
    "chunk_overlap_size = config['chunk_overlap']\n",
    "\n",
    "# Separators\n",
    "separators = [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use recursive splitter, as it has been proved to be the best performing splitter for problems like ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chunk_1</td>\n",
       "      <td>LEY ORGÁNICA 10/1995, DE 23 DE NOVIEMBRE, DEL\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chunk_2</td>\n",
       "      <td>JUAN CARLOS I\\nRey de España\\nA todos los que ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chunk_3</td>\n",
       "      <td>negativa». El Código Penal ha de tutelar los v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chunk_4</td>\n",
       "      <td>sucinto, los criterios en que se inspira, aunq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chunk_5</td>\n",
       "      <td>menos básicos, y, de otra, introduce cambios e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_id                                               text\n",
       "0  chunk_1  LEY ORGÁNICA 10/1995, DE 23 DE NOVIEMBRE, DEL\\...\n",
       "1  chunk_2  JUAN CARLOS I\\nRey de España\\nA todos los que ...\n",
       "2  chunk_3  negativa». El Código Penal ha de tutelar los v...\n",
       "3  chunk_4  sucinto, los criterios en que se inspira, aunq...\n",
       "4  chunk_5  menos básicos, y, de otra, introduce cambios e..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=max_chunk_size,\n",
    "    chunk_overlap=chunk_overlap_size,\n",
    "    length_function=len,\n",
    "    separators=separators\n",
    ")\n",
    "\n",
    "# Auxiliar function\n",
    "def recursive_text_splitter(text):\n",
    "    chunks = splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "# Split the text into chunks\n",
    "chunks = recursive_text_splitter(df_txt)\n",
    "\n",
    "# Create a list of dictionaries containing chunked data\n",
    "chunks_list = [{'text_id': f\"chunk_{i+1}\", 'text': chunk} for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# Create a DataFrame from the chunks list\n",
    "splitted_df_v1 = pd.DataFrame(chunks_list)\n",
    "splitted_df_v1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1068"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Length of file/\n",
    "len(splitted_df_v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now save the already splitted data in folder prepared_data. We will later use it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save splitted data\n",
    "path = 'prepared_data/'\n",
    "csv_file_name_v1 = f'{path}splitted_input_base.csv'\n",
    "splitted_df_v1.to_csv(csv_file_name_v1, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Count & Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the total number of tokens that we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in the 'text' column: 238500\n"
     ]
    }
   ],
   "source": [
    "# Sum total tokens\n",
    "total_tokens_processed = splitted_df_v1['text'].apply(lambda x: count_tokens(x)).sum()\n",
    "\n",
    "# Show\n",
    "print(\"Total number of tokens in the 'text' column:\", total_tokens_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the runtime from the recursive splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0 hours and 0 minutes.\n"
     ]
    }
   ],
   "source": [
    "# End time of notebook run\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Convert elapsed time to hours and minutes\n",
    "hours = int(elapsed_time // 3600)\n",
    "minutes = int((elapsed_time % 3600) // 60)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Time elapsed: {hours} hours and {minutes} minutes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
