{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bac8571-0c20-4b76-9b65-ee83d704a3fc",
   "metadata": {},
   "source": [
    "# Raptor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bb0118-0767-4dd4-a05e-55dad3676a51",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2452e26c-1b44-4a8a-a205-4e74890aa4dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T10:52:16.154631Z",
     "iopub.status.busy": "2024-03-26T10:52:16.154217Z",
     "iopub.status.idle": "2024-03-26T10:52:16.180841Z",
     "shell.execute_reply": "2024-03-26T10:52:16.180295Z",
     "shell.execute_reply.started": "2024-03-26T10:52:16.154605Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Environment Variables\n",
    "from dotenv import load_dotenv\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "# Load env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37dc845f-ec01-4155-8043-b18ec0f21ae0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T10:52:16.182261Z",
     "iopub.status.busy": "2024-03-26T10:52:16.181860Z",
     "iopub.status.idle": "2024-03-26T10:52:17.464826Z",
     "shell.execute_reply": "2024-03-26T10:52:17.464271Z",
     "shell.execute_reply.started": "2024-03-26T10:52:16.182243Z"
    }
   },
   "outputs": [],
   "source": [
    "# Torch config\n",
    "from torch import cuda, bfloat16, float16\n",
    "import torch\n",
    "\n",
    "# Torch options\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fc2e3ce-565a-4724-b994-859c06766ebb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T10:52:17.466141Z",
     "iopub.status.busy": "2024-03-26T10:52:17.465697Z",
     "iopub.status.idle": "2024-03-26T10:52:17.920333Z",
     "shell.execute_reply": "2024-03-26T10:52:17.919695Z",
     "shell.execute_reply.started": "2024-03-26T10:52:17.466113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Jupyter extensions\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4b832d-2996-4588-bf7f-87b154f29fa8",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8304380-b489-40c7-b1dc-e32d84d1b29c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T10:52:17.922854Z",
     "iopub.status.busy": "2024-03-26T10:52:17.922301Z",
     "iopub.status.idle": "2024-03-26T10:52:17.932615Z",
     "shell.execute_reply": "2024-03-26T10:52:17.932050Z",
     "shell.execute_reply.started": "2024-03-26T10:52:17.922824Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load parameters from YAML file\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7644e779-06e3-4b10-a2da-143f6c6a9804",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T10:52:17.933666Z",
     "iopub.status.busy": "2024-03-26T10:52:17.933385Z",
     "iopub.status.idle": "2024-03-26T10:52:17.938903Z",
     "shell.execute_reply": "2024-03-26T10:52:17.938374Z",
     "shell.execute_reply.started": "2024-03-26T10:52:17.933640Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use optimum\n",
    "use_optimum = config[\"use_optimum\"]\n",
    "\n",
    "# Show\n",
    "use_optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493230d1-0331-41d4-bf43-896fb956e9d3",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45472380-f53a-4b50-b8e8-83202f4757f9",
   "metadata": {},
   "source": [
    "- https://github.com/parthsarthi03/raptor/tree/master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e72c38a-4416-4b08-bba2-a23161431db3",
   "metadata": {},
   "source": [
    "# Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dba282e-f796-44ab-8433-b9e2f6d1e34d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T10:52:17.940238Z",
     "iopub.status.busy": "2024-03-26T10:52:17.939675Z",
     "iopub.status.idle": "2024-03-26T10:52:17.944700Z",
     "shell.execute_reply": "2024-03-26T10:52:17.944192Z",
     "shell.execute_reply.started": "2024-03-26T10:52:17.940211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/workspace/LawGPT'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set directory to file location\n",
    "from pathlib import Path\n",
    "import sys\n",
    "notebook_location = Path(os.path.abspath(''))\n",
    "os.chdir(notebook_location)\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "current_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c333005-54c3-4009-9a9e-6609d53a4504",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c40960ae-b599-4b12-a826-441d989040b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T10:52:17.945525Z",
     "iopub.status.busy": "2024-03-26T10:52:17.945328Z",
     "iopub.status.idle": "2024-03-26T10:52:31.327936Z",
     "shell.execute_reply": "2024-03-26T10:52:31.327355Z",
     "shell.execute_reply.started": "2024-03-26T10:52:17.945508Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-26 10:52:31,080 - Loading faiss with AVX2 support.\n",
      "2024-03-26 10:52:31,171 - Successfully loaded faiss with AVX2 support.\n"
     ]
    }
   ],
   "source": [
    "# General libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import yaml\n",
    "import csv\n",
    "import sys\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import transformers\n",
    "import accelerate\n",
    "\n",
    "# Splitters\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "# Raptor\n",
    "from raptor import RetrievalAugmentation, BaseSummarizationModel, BaseQAModel, BaseEmbeddingModel, RetrievalAugmentationConfig\n",
    "\n",
    "# Tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Optimization\n",
    "import xformers\n",
    "\n",
    "# Cloud\n",
    "from google.cloud import storage\n",
    "\n",
    "# Other\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Local\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "340b9550-d169-42c3-9b6b-bce7790811b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T10:52:31.329177Z",
     "iopub.status.busy": "2024-03-26T10:52:31.328678Z",
     "iopub.status.idle": "2024-03-26T10:52:31.331833Z",
     "shell.execute_reply": "2024-03-26T10:52:31.331228Z",
     "shell.execute_reply.started": "2024-03-26T10:52:31.329157Z"
    }
   },
   "outputs": [],
   "source": [
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e455c48-350c-4659-831f-d004af88b72f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T10:52:31.332958Z",
     "iopub.status.busy": "2024-03-26T10:52:31.332546Z",
     "iopub.status.idle": "2024-03-26T10:52:31.337272Z",
     "shell.execute_reply": "2024-03-26T10:52:31.336751Z",
     "shell.execute_reply.started": "2024-03-26T10:52:31.332915Z"
    }
   },
   "outputs": [],
   "source": [
    "# Start timing the notebook run\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74054cdf-fc91-40a1-9267-b7c993dc840c",
   "metadata": {},
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e983776-68b0-4960-9fb3-4d6b8bda8917",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T10:52:31.340071Z",
     "iopub.status.busy": "2024-03-26T10:52:31.339577Z",
     "iopub.status.idle": "2024-03-26T10:52:31.348019Z",
     "shell.execute_reply": "2024-03-26T10:52:31.347448Z",
     "shell.execute_reply.started": "2024-03-26T10:52:31.340052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA A10G\n",
      "Memory Usage:\n",
      "Allocated:    0.0 GB\n",
      "Cached:       0.0 GB\n",
      "Available:   22.0 GB\n",
      "Total:       22.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "# CUDA information\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    allocated_memory = torch.cuda.memory_allocated(0) / (1024**3)  # Convert bytes to GB\n",
    "    cached_memory = torch.cuda.memory_reserved(0) / (1024**3)  # Convert bytes to GB\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Convert bytes to GB\n",
    "    available_memory = total_memory - cached_memory\n",
    "    print('Allocated:   ', round(allocated_memory, 1), 'GB')\n",
    "    print('Cached:      ', round(cached_memory, 1), 'GB')\n",
    "    print('Available:  ', round(available_memory, 1), 'GB')\n",
    "    print('Total:      ', round(total_memory, 1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3efee2-111d-48a3-8413-4328d5462e1c",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "893bf411-b3d8-48a9-8afb-9aae497d26fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T10:52:31.348890Z",
     "iopub.status.busy": "2024-03-26T10:52:31.348721Z",
     "iopub.status.idle": "2024-03-26T10:52:31.368835Z",
     "shell.execute_reply": "2024-03-26T10:52:31.368282Z",
     "shell.execute_reply.started": "2024-03-26T10:52:31.348873Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>legislative_origin</th>\n",
       "      <th>department</th>\n",
       "      <th>rang</th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1995-25444</td>\n",
       "      <td>https://www.boe.es/diario_boe/xml.php?id=BOE-A...</td>\n",
       "      <td>Ley Orgánica 10/1995, de 23 de noviembre, del ...</td>\n",
       "      <td>1995-11-24</td>\n",
       "      <td>Estatal</td>\n",
       "      <td>Jefatura del Estado</td>\n",
       "      <td>Ley Orgánica</td>\n",
       "      <td>1995-25444_chunk1</td>\n",
       "      <td>JUAN CARLOS I\\nREY DE ESPAÑA\\nA todos los que ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1995-25444</td>\n",
       "      <td>https://www.boe.es/diario_boe/xml.php?id=BOE-A...</td>\n",
       "      <td>Ley Orgánica 10/1995, de 23 de noviembre, del ...</td>\n",
       "      <td>1995-11-24</td>\n",
       "      <td>Estatal</td>\n",
       "      <td>Jefatura del Estado</td>\n",
       "      <td>Ley Orgánica</td>\n",
       "      <td>1995-25444_chunk2</td>\n",
       "      <td>EXPOSICION DE MOTIVOS\\nSi se ha llegado a defi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1995-25444</td>\n",
       "      <td>https://www.boe.es/diario_boe/xml.php?id=BOE-A...</td>\n",
       "      <td>Ley Orgánica 10/1995, de 23 de noviembre, del ...</td>\n",
       "      <td>1995-11-24</td>\n",
       "      <td>Estatal</td>\n",
       "      <td>Jefatura del Estado</td>\n",
       "      <td>Ley Orgánica</td>\n",
       "      <td>1995-25444_chunk3</td>\n",
       "      <td>A partir de los distintos intentos de reforma ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1995-25444</td>\n",
       "      <td>https://www.boe.es/diario_boe/xml.php?id=BOE-A...</td>\n",
       "      <td>Ley Orgánica 10/1995, de 23 de noviembre, del ...</td>\n",
       "      <td>1995-11-24</td>\n",
       "      <td>Estatal</td>\n",
       "      <td>Jefatura del Estado</td>\n",
       "      <td>Ley Orgánica</td>\n",
       "      <td>1995-25444_chunk4</td>\n",
       "      <td>El eje de dichos criterios ha sido, como es ló...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1995-25444</td>\n",
       "      <td>https://www.boe.es/diario_boe/xml.php?id=BOE-A...</td>\n",
       "      <td>Ley Orgánica 10/1995, de 23 de noviembre, del ...</td>\n",
       "      <td>1995-11-24</td>\n",
       "      <td>Estatal</td>\n",
       "      <td>Jefatura del Estado</td>\n",
       "      <td>Ley Orgánica</td>\n",
       "      <td>1995-25444_chunk5</td>\n",
       "      <td>En segundo lugar, se ha afrontado la antinomia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                                url  \\\n",
       "0  1995-25444  https://www.boe.es/diario_boe/xml.php?id=BOE-A...   \n",
       "1  1995-25444  https://www.boe.es/diario_boe/xml.php?id=BOE-A...   \n",
       "2  1995-25444  https://www.boe.es/diario_boe/xml.php?id=BOE-A...   \n",
       "3  1995-25444  https://www.boe.es/diario_boe/xml.php?id=BOE-A...   \n",
       "4  1995-25444  https://www.boe.es/diario_boe/xml.php?id=BOE-A...   \n",
       "\n",
       "                                               title        date  \\\n",
       "0  Ley Orgánica 10/1995, de 23 de noviembre, del ...  1995-11-24   \n",
       "1  Ley Orgánica 10/1995, de 23 de noviembre, del ...  1995-11-24   \n",
       "2  Ley Orgánica 10/1995, de 23 de noviembre, del ...  1995-11-24   \n",
       "3  Ley Orgánica 10/1995, de 23 de noviembre, del ...  1995-11-24   \n",
       "4  Ley Orgánica 10/1995, de 23 de noviembre, del ...  1995-11-24   \n",
       "\n",
       "  legislative_origin           department          rang            text_id  \\\n",
       "0            Estatal  Jefatura del Estado  Ley Orgánica  1995-25444_chunk1   \n",
       "1            Estatal  Jefatura del Estado  Ley Orgánica  1995-25444_chunk2   \n",
       "2            Estatal  Jefatura del Estado  Ley Orgánica  1995-25444_chunk3   \n",
       "3            Estatal  Jefatura del Estado  Ley Orgánica  1995-25444_chunk4   \n",
       "4            Estatal  Jefatura del Estado  Ley Orgánica  1995-25444_chunk5   \n",
       "\n",
       "                                                text  \n",
       "0  JUAN CARLOS I\\nREY DE ESPAÑA\\nA todos los que ...  \n",
       "1  EXPOSICION DE MOTIVOS\\nSi se ha llegado a defi...  \n",
       "2  A partir de los distintos intentos de reforma ...  \n",
       "3  El eje de dichos criterios ha sido, como es ló...  \n",
       "4  En segundo lugar, se ha afrontado la antinomia...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set file path with complete data\n",
    "file_path = 'prepared_data/splitted_input_base.csv'\n",
    "\n",
    "# Read the data into a DataFrame\n",
    "df_txt = pd.read_csv(file_path)\n",
    "\n",
    "# Show\n",
    "df_txt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f1fb0e-1ddc-49b7-82a8-315f0bc41d8a",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e70b8c6-14d9-4473-b44a-98229c6cf23c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T10:52:31.369904Z",
     "iopub.status.busy": "2024-03-26T10:52:31.369562Z",
     "iopub.status.idle": "2024-03-26T10:52:31.378851Z",
     "shell.execute_reply": "2024-03-26T10:52:31.378272Z",
     "shell.execute_reply.started": "2024-03-26T10:52:31.369886Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load parameters from YAML file\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf81677d-37be-45f9-871d-bcfa57014d7a",
   "metadata": {},
   "source": [
    "# Custom model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a127ed-d4ef-43ef-b3ce-94b5e72143e6",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72e0fc1a-fc4f-453c-824e-bebcb76b5e0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T10:52:31.379908Z",
     "iopub.status.busy": "2024-03-26T10:52:31.379541Z",
     "iopub.status.idle": "2024-03-26T10:52:31.384719Z",
     "shell.execute_reply": "2024-03-26T10:52:31.384149Z",
     "shell.execute_reply.started": "2024-03-26T10:52:31.379889Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model ID\n",
    "use_quantization = config[\"core_quantization\"]\n",
    "\n",
    "# Show\n",
    "use_quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57f3609-9dbf-41bf-acfd-31c7d534a1c9",
   "metadata": {},
   "source": [
    "## Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45acf261-2e71-4bdd-8ee8-afeeed91f8a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T10:52:31.385647Z",
     "iopub.status.busy": "2024-03-26T10:52:31.385353Z",
     "iopub.status.idle": "2024-03-26T10:52:31.390344Z",
     "shell.execute_reply": "2024-03-26T10:52:31.389790Z",
     "shell.execute_reply.started": "2024-03-26T10:52:31.385630Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sentence-transformers/multi-qa-mpnet-base-cos-v1'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model ID\n",
    "embed_model_id = config[\"embedding_model\"]\n",
    "\n",
    "# Show\n",
    "embed_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5a6781e-0e43-4f6e-b449-38e5f4b29a4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T10:52:31.391330Z",
     "iopub.status.busy": "2024-03-26T10:52:31.391053Z",
     "iopub.status.idle": "2024-03-26T10:52:31.394711Z",
     "shell.execute_reply": "2024-03-26T10:52:31.394185Z",
     "shell.execute_reply.started": "2024-03-26T10:52:31.391314Z"
    }
   },
   "outputs": [],
   "source": [
    "# Embedding class\n",
    "class Embedding_Model(BaseEmbeddingModel):\n",
    "    \n",
    "    def __init__(self, model_name):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def create_embedding(self, text):\n",
    "        return self.model.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69bdb7a-1c9d-4b7a-aead-07152523f463",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4426ac1b-572c-4a18-84d5-915de1369811",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T10:52:31.395625Z",
     "iopub.status.busy": "2024-03-26T10:52:31.395351Z",
     "iopub.status.idle": "2024-03-26T10:52:31.400683Z",
     "shell.execute_reply": "2024-03-26T10:52:31.400101Z",
     "shell.execute_reply.started": "2024-03-26T10:52:31.395608Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'google/gemma-2b-it'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarizer\n",
    "core_model_id = config['core_model']\n",
    "\n",
    "# Show\n",
    "core_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e233476-4aa4-490f-ba1b-5987c6e6fca0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T10:52:31.401639Z",
     "iopub.status.busy": "2024-03-26T10:52:31.401357Z",
     "iopub.status.idle": "2024-03-26T10:52:31.407768Z",
     "shell.execute_reply": "2024-03-26T10:52:31.407187Z",
     "shell.execute_reply.started": "2024-03-26T10:52:31.401622Z"
    }
   },
   "outputs": [],
   "source": [
    "# Summarization class\n",
    "class Summarization_Model(BaseSummarizationModel):\n",
    "    \n",
    "    def __init__(self, model_name, use_quantization = True):\n",
    "        \n",
    "        # Initialize the tokenizer and the pipeline for the model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        if use_quantization:\n",
    "            # Quantization\n",
    "            self.bnb_config = transformers.BitsAndBytesConfig(\n",
    "                load_in_4bit = True,\n",
    "                bnb_4bit_quant_type = 'nf4',\n",
    "                bnb_4bit_use_double_quant = True,\n",
    "                bnb_4bit_compute_dtype = torch.bfloat16\n",
    "            )\n",
    "        else:\n",
    "            self.bnb_config = None\n",
    "        \n",
    "        # Set model\n",
    "        self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code = True,\n",
    "            quantization_config = self.bnb_config,\n",
    "            device_map = \"auto\"\n",
    "        )\n",
    "\n",
    "        # Create pipeline\n",
    "        self.summarization_pipeline = transformers.pipeline(\n",
    "            model = self.model,\n",
    "            tokenizer = self.tokenizer,\n",
    "            task = 'text-generation',\n",
    "            model_kwargs = {\"torch_dtype\": torch.bfloat16},\n",
    "            return_full_text = config[\"core_return_full_text\"],\n",
    "            max_new_tokens = config[\"core_max_new_tokens\"],\n",
    "            repetition_penalty = config[\"core_repetition_penalty\"],\n",
    "            temperature = config[\"core_temperature\"],\n",
    "            pad_token_id = self.tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    def summarize(self, context):\n",
    "        \n",
    "        # Format the prompt for summarization\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"Write a summary of the following, including as many key details as possible: {context}:\"}\n",
    "        ]\n",
    "        \n",
    "        # Tokenizer for chat\n",
    "        prompt = self.tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\n",
    "        \n",
    "        # Generate the summary using the pipeline\n",
    "        outputs = self.summarization_pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens = config[\"core_max_new_tokens\"],\n",
    "            do_sample = True,\n",
    "            temperature = 0.5,\n",
    "            top_k = 50,\n",
    "            top_p = 0.95\n",
    "        )\n",
    "        \n",
    "        # Extracting and returning the generated summary\n",
    "        summary = outputs[0][\"generated_text\"].strip()\n",
    "        \n",
    "        # Return\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07016a3f-286a-43b4-ab94-b4427957083f",
   "metadata": {},
   "source": [
    "## QA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a22c0f28-4712-437c-a6d9-1c53ea8f7533",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T10:52:31.409016Z",
     "iopub.status.busy": "2024-03-26T10:52:31.408676Z",
     "iopub.status.idle": "2024-03-26T10:52:31.415702Z",
     "shell.execute_reply": "2024-03-26T10:52:31.415178Z",
     "shell.execute_reply.started": "2024-03-26T10:52:31.408999Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'google/gemma-2b-it'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarizer\n",
    "core_model_id = config['core_model']\n",
    "\n",
    "# Show\n",
    "core_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dc9271f-f9c3-4de8-b16d-df5d784c7915",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T10:52:31.416689Z",
     "iopub.status.busy": "2024-03-26T10:52:31.416395Z",
     "iopub.status.idle": "2024-03-26T10:52:31.422930Z",
     "shell.execute_reply": "2024-03-26T10:52:31.422384Z",
     "shell.execute_reply.started": "2024-03-26T10:52:31.416672Z"
    }
   },
   "outputs": [],
   "source": [
    "# QA class\n",
    "class QA_Model(BaseQAModel):\n",
    "    \n",
    "    def __init__(self, model_name, use_quantization = True):\n",
    "        \n",
    "        # Initialize the tokenizer and the pipeline for the model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        if use_quantization:\n",
    "            # Quantization\n",
    "            self.bnb_config = transformers.BitsAndBytesConfig(\n",
    "                load_in_4bit = True,\n",
    "                bnb_4bit_quant_type = 'nf4',\n",
    "                bnb_4bit_use_double_quant = True,\n",
    "                bnb_4bit_compute_dtype = torch.bfloat16\n",
    "            )\n",
    "        else:\n",
    "            self.bnb_config = None\n",
    "        \n",
    "        # Set model\n",
    "        self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code = True,\n",
    "            quantization_config = self.bnb_config,\n",
    "            device_map = \"auto\"\n",
    "        )\n",
    "\n",
    "        # Create pipeline\n",
    "        self.qa_pipeline = transformers.pipeline(\n",
    "            model = self.model,\n",
    "            tokenizer = self.tokenizer,\n",
    "            task = 'text-generation',\n",
    "            model_kwargs = {\"torch_dtype\": torch.bfloat16},\n",
    "            return_full_text = config[\"core_return_full_text\"],\n",
    "            max_new_tokens = config[\"core_max_new_tokens\"],\n",
    "            repetition_penalty = config[\"core_repetition_penalty\"],\n",
    "            temperature = config[\"core_temperature\"],\n",
    "            pad_token_id = self.tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    def answer_question(self, context, question):\n",
    "        \n",
    "        # Apply the chat template for the context and question\n",
    "        messages=[\n",
    "              {\"role\": \"user\", \"content\": f\"Dado el contexto: {context} Da la mejor respuesta completa entre las opciones a la pregunta {question}\"}\n",
    "        ]\n",
    "        \n",
    "        # pROMPT\n",
    "        prompt = self.tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = True)\n",
    "        \n",
    "        # Generate the answer using the pipeline\n",
    "        outputs = self.qa_pipeline(\n",
    "            prompt,\n",
    "            max_new_tokens = config[\"core_max_new_tokens\"],\n",
    "            do_sample = True,\n",
    "            temperature = 0.5,\n",
    "            top_k = 50,\n",
    "            top_p = 0.95\n",
    "        )\n",
    "        \n",
    "        # Extracting and returning the generated answer\n",
    "        answer = outputs[0][\"generated_text\"][len(prompt):]\n",
    "        \n",
    "        # Return\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b85721-c6bb-4359-8698-2be5c9bd7f4b",
   "metadata": {},
   "source": [
    "# Custom RAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a77a5b2-5199-4d96-b306-4bc083245fae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T10:52:31.423865Z",
     "iopub.status.busy": "2024-03-26T10:52:31.423554Z",
     "iopub.status.idle": "2024-03-26T10:52:38.318146Z",
     "shell.execute_reply": "2024-03-26T10:52:38.317564Z",
     "shell.execute_reply.started": "2024-03-26T10:52:31.423847Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-26 10:52:31,428 - Load pretrained SentenceTransformer: sentence-transformers/multi-qa-mpnet-base-cos-v1\n",
      "2024-03-26 10:52:31,632 - Use pytorch device: cuda\n",
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "2024-03-26 10:52:33,201 - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a647cf060d4145518a29fa8bbbabe0e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-26 10:52:36,173 - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0093bbd15d848d5903ba3b885a610f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<raptor.RetrievalAugmentation.RetrievalAugmentationConfig at 0x7f2683a12110>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set custom RAC\n",
    "RAC = RetrievalAugmentationConfig(\n",
    "    embedding_model = Embedding_Model(embed_model_id),\n",
    "    summarization_model = Summarization_Model(core_model_id, use_quantization), \n",
    "    qa_model = QA_Model(core_model_id, use_quantization)\n",
    ")\n",
    "\n",
    "# Show\n",
    "RAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ed5a57e-4f54-40aa-861b-1618e522ddc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T10:52:38.319297Z",
     "iopub.status.busy": "2024-03-26T10:52:38.318933Z",
     "iopub.status.idle": "2024-03-26T10:52:38.324959Z",
     "shell.execute_reply": "2024-03-26T10:52:38.324423Z",
     "shell.execute_reply.started": "2024-03-26T10:52:38.319278Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-26 10:52:38,320 - Successfully initialized TreeBuilder with Config \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <__main__.Summarization_Model object at 0x7f2683ef9ff0>\n",
      "            Embedding Models: {'EMB': <__main__.Embedding_Model object at 0x7f2683ef94e0>}\n",
      "            Cluster Embedding Model: EMB\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "2024-03-26 10:52:38,320 - Successfully initialized ClusterTreeBuilder with Config \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <__main__.Summarization_Model object at 0x7f2683ef9ff0>\n",
      "            Embedding Models: {'EMB': <__main__.Embedding_Model object at 0x7f2683ef94e0>}\n",
      "            Cluster Embedding Model: EMB\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "2024-03-26 10:52:38,321 - Successfully initialized RetrievalAugmentation with Config \n",
      "        RetrievalAugmentationConfig:\n",
      "            \n",
      "        TreeBuilderConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Max Tokens: 100\n",
      "            Num Layers: 5\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Summarization Length: 100\n",
      "            Summarization Model: <__main__.Summarization_Model object at 0x7f2683ef9ff0>\n",
      "            Embedding Models: {'EMB': <__main__.Embedding_Model object at 0x7f2683ef94e0>}\n",
      "            Cluster Embedding Model: EMB\n",
      "        \n",
      "        Reduction Dimension: 10\n",
      "        Clustering Algorithm: RAPTOR_Clustering\n",
      "        Clustering Parameters: {}\n",
      "        \n",
      "            \n",
      "            \n",
      "        TreeRetrieverConfig:\n",
      "            Tokenizer: <Encoding 'cl100k_base'>\n",
      "            Threshold: 0.5\n",
      "            Top K: 5\n",
      "            Selection Mode: top_k\n",
      "            Context Embedding Model: EMB\n",
      "            Embedding Model: <__main__.Embedding_Model object at 0x7f2683ef94e0>\n",
      "            Num Layers: None\n",
      "            Start Layer: None\n",
      "        \n",
      "            \n",
      "            QA Model: <__main__.QA_Model object at 0x7f2683bbfe80>\n",
      "            Tree Builder Type: cluster\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<raptor.RetrievalAugmentation.RetrievalAugmentation at 0x7f26837148b0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create RA\n",
    "RA = RetrievalAugmentation(config = RAC)\n",
    "\n",
    "# Show\n",
    "RA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c28d2045-195a-46e5-bc14-a8b449907425",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T10:52:38.326014Z",
     "iopub.status.busy": "2024-03-26T10:52:38.325694Z",
     "iopub.status.idle": "2024-03-26T10:52:38.331337Z",
     "shell.execute_reply": "2024-03-26T10:52:38.330758Z",
     "shell.execute_reply.started": "2024-03-26T10:52:38.325996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A10G\n",
      "Memory Usage:\n",
      "Allocated: 3.9 GB\n",
      "Cached:    3.9 GB\n",
      "Total:      22.0 GB\n",
      "Available:  18.1 GB\n"
     ]
    }
   ],
   "source": [
    "# CUDA information\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3, 1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3, 1), 'GB')\n",
    "    properties = torch.cuda.get_device_properties(0)\n",
    "    total_memory = properties.total_memory / 1024**3  # Convert bytes to GB\n",
    "    allocated_memory = torch.cuda.memory_allocated(0) / 1024**3  # Convert bytes to GB\n",
    "    available_memory = total_memory - allocated_memory\n",
    "    print('Total:     ', round(total_memory, 1), 'GB')\n",
    "    print('Available: ', round(available_memory, 1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faefd5a7-7365-45a0-be7c-8c362865edae",
   "metadata": {},
   "source": [
    "# Add documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "717f4f3d-a423-4acd-9e35-017920c460cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T10:52:38.332378Z",
     "iopub.status.busy": "2024-03-26T10:52:38.332036Z",
     "iopub.status.idle": "2024-03-26T10:52:38.337937Z",
     "shell.execute_reply": "2024-03-26T10:52:38.337425Z",
     "shell.execute_reply.started": "2024-03-26T10:52:38.332358Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example run\n",
    "with open('raw_data/sample.txt', 'r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296c80c3-4582-4d06-afa9-d8d6018017bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-26T10:52:38.338921Z",
     "iopub.status.busy": "2024-03-26T10:52:38.338612Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-26 10:52:38,346 - Creating Leaf Nodes\n"
     ]
    }
   ],
   "source": [
    "# Add documents\n",
    "RA.add_documents(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85986fd2-6c3e-4bda-aa37-b27dc0a29cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA information\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3, 1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3, 1), 'GB')\n",
    "    properties = torch.cuda.get_device_properties(0)\n",
    "    total_memory = properties.total_memory / 1024**3  # Convert bytes to GB\n",
    "    allocated_memory = torch.cuda.memory_allocated(0) / 1024**3  # Convert bytes to GB\n",
    "    available_memory = total_memory - allocated_memory\n",
    "    print('Total:     ', round(total_memory, 1), 'GB')\n",
    "    print('Available: ', round(available_memory, 1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c020636-0637-4585-a1b6-bc243cd6f601",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6999c73-fbba-48c4-a308-35e74c917958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query\n",
    "# query = \"Explícame el Artículo 245 del Código Penal de España\"\n",
    "query = \"How did Cinderella reach her happy ending?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3949145-9697-4339-9d77-9c2edabf64e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get asnwer\n",
    "answer = RA.answer_question(question = query)\n",
    "\n",
    "# Show\n",
    "print(\"Answer: \", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688590c0-6351-491a-9a78-7541bd6f4d6a",
   "metadata": {},
   "source": [
    "# Save tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fb6c16-ce29-49e1-b56a-605f99d223c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save path\n",
    "save_path = \"raptor_dir/test\"\n",
    "\n",
    "# Save\n",
    "RA.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012123ee-fa99-41b6-8ca5-f5f530fffe0a",
   "metadata": {},
   "source": [
    "# Load tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40fcdce-21d4-43a1-bbd8-956f51075b54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load tree\n",
    "RA = RetrievalAugmentation(tree = save_path)\n",
    "\n",
    "# Show\n",
    "RA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafb9943-1fd8-450b-a0ca-07cd63276911",
   "metadata": {},
   "source": [
    "# Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47824d16-49d6-4df8-93fb-bfe9a2f0e6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA information\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    allocated_memory = torch.cuda.memory_allocated(0) / (1024**3)  # Convert bytes to GB\n",
    "    cached_memory = torch.cuda.memory_reserved(0) / (1024**3)  # Convert bytes to GB\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Convert bytes to GB\n",
    "    available_memory = total_memory - cached_memory\n",
    "    print('Allocated:   ', round(allocated_memory, 1), 'GB')\n",
    "    print('Cached:      ', round(cached_memory, 1), 'GB')\n",
    "    print('Available:  ', round(available_memory, 1), 'GB')\n",
    "    print('Total:      ', round(total_memory, 1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec31f4f1-6aca-4a0d-ab2f-66648419627d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
