{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Environment Variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set env path\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"lawgpt-410122-140fba6fba7b.json\"\n",
    "\n",
    "# Bucket params\n",
    "bucket_name = 'lawgpt_madrid_bucket_1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@ayhamboucher/llm-based-context-splitter-for-large-documents-445d3f02b01b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.mongodb.com/developer/products/atlas/gemma-mongodb-huggingface-rag/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.pinecone.io/learn/chunking-strategies/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/notebooks/LawGPT'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set directory to file location\n",
    "from pathlib import Path\n",
    "import sys\n",
    "notebook_location = Path(os.path.abspath(''))\n",
    "os.chdir(notebook_location)\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "current_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import yaml\n",
    "import csv\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import transformers\n",
    "import accelerate\n",
    "\n",
    "# Splitters\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "# Torch\n",
    "from torch import cuda, bfloat16, float16\n",
    "import torch\n",
    "\n",
    "# Cloud\n",
    "from google.cloud import storage\n",
    "\n",
    "# Other\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Local\n",
    "from functions import *\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing the notebook run\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Platform login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Quadro P5000\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "# CUDA information\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['boe_ids.csv', 'boe_filtered_ids.csv', 'boe_data.csv']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Params\n",
    "file_list = [\"boe_data.csv\"]\n",
    "local_folder_path = \"raw_data/\"\n",
    "\n",
    "# List CSV files locally\n",
    "local_csv_files = list_csv_files(local_folder_path)\n",
    "local_csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded boe_data.csv from local storage.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>legislative_origin</th>\n",
       "      <th>department</th>\n",
       "      <th>rang</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1995-25444</td>\n",
       "      <td>https://www.boe.es/diario_boe/xml.php?id=BOE-A...</td>\n",
       "      <td>Ley Orgánica 10/1995, de 23 de noviembre, del ...</td>\n",
       "      <td>1995-11-24</td>\n",
       "      <td>Estatal</td>\n",
       "      <td>Jefatura del Estado</td>\n",
       "      <td>Ley Orgánica</td>\n",
       "      <td>\\nJUAN CARLOS I\\nREY DE ESPAÑA\\nA todos los qu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                                url  \\\n",
       "0  1995-25444  https://www.boe.es/diario_boe/xml.php?id=BOE-A...   \n",
       "\n",
       "                                               title        date  \\\n",
       "0  Ley Orgánica 10/1995, de 23 de noviembre, del ...  1995-11-24   \n",
       "\n",
       "  legislative_origin           department          rang  \\\n",
       "0            Estatal  Jefatura del Estado  Ley Orgánica   \n",
       "\n",
       "                                                text  \n",
       "0  \\nJUAN CARLOS I\\nREY DE ESPAÑA\\nA todos los qu...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for each file in file_list\n",
    "df_txt = pd.DataFrame()\n",
    "\n",
    "# Loop by files\n",
    "for file_name in file_list:\n",
    "    try:\n",
    "        if file_name in local_csv_files:\n",
    "            # If file found locally, load and concatenate\n",
    "            local_file_path = os.path.join(local_folder_path, file_name)\n",
    "            df = pd.read_csv(local_file_path)\n",
    "            df['id'] = df['id'].astype(str)\n",
    "            print(f\"Loaded {file_name} from local storage.\")\n",
    "        else:\n",
    "            # If file not found locally, download from GCS and then load\n",
    "            # source_blob_name = f'raw_data/{file_name}'\n",
    "            # destination_file_name = os.path.join(local_folder_path, file_name)\n",
    "            # download_from_gcs(os.environ.get('GOOGLE_BUCKET_NAME'), source_blob_name, destination_file_name)\n",
    "            # df = pd.read_csv(destination_file_name)\n",
    "            print(f\"Downloaded and loaded {file_name} from GCS.\")\n",
    "\n",
    "        # Concatenate or process the DataFrame as needed\n",
    "        df_txt = pd.concat([df_txt, df], ignore_index=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df_txt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Length of file\n",
    "len(df_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the character count for each row in the specified column\n",
    "df_txt['character_count'] = df_txt['text'].apply(lambda x: len(str(x)) if pd.notna(x) else 0)\n",
    "\n",
    "# Filter rows based on the character count condition\n",
    "df_txt = df_txt[df_txt['character_count'] > 0].copy()\n",
    "\n",
    "# Drop the temporary 'character_count' column\n",
    "df_txt = df_txt.drop(columns=['character_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Length of file\n",
    "len(df_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters from YAML file\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Maximum length of a text\n",
    "max_chunk_size = config['max_chunk_size']\n",
    "\n",
    "# Chunk overlap\n",
    "chunk_overlap_size = config['chunk_overlap']\n",
    "\n",
    "# Separators\n",
    "separators = [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Recursive Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = max_chunk_size,\n",
    "    chunk_overlap  = chunk_overlap_size,\n",
    "    length_function = len,\n",
    "    separators = separators\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliar function\n",
    "def recursive_text_splitter(text):\n",
    "    chunks = splitter.split_text(text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>legislative_origin</th>\n",
       "      <th>department</th>\n",
       "      <th>rang</th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1995-25444</td>\n",
       "      <td>https://www.boe.es/diario_boe/xml.php?id=BOE-A...</td>\n",
       "      <td>Ley Orgánica 10/1995, de 23 de noviembre, del ...</td>\n",
       "      <td>1995-11-24</td>\n",
       "      <td>Estatal</td>\n",
       "      <td>Jefatura del Estado</td>\n",
       "      <td>Ley Orgánica</td>\n",
       "      <td>1995-25444_chunk1</td>\n",
       "      <td>JUAN CARLOS I\\nREY DE ESPAÑA\\nA todos los que ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1995-25444</td>\n",
       "      <td>https://www.boe.es/diario_boe/xml.php?id=BOE-A...</td>\n",
       "      <td>Ley Orgánica 10/1995, de 23 de noviembre, del ...</td>\n",
       "      <td>1995-11-24</td>\n",
       "      <td>Estatal</td>\n",
       "      <td>Jefatura del Estado</td>\n",
       "      <td>Ley Orgánica</td>\n",
       "      <td>1995-25444_chunk2</td>\n",
       "      <td>A partir de los distintos intentos de reforma ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1995-25444</td>\n",
       "      <td>https://www.boe.es/diario_boe/xml.php?id=BOE-A...</td>\n",
       "      <td>Ley Orgánica 10/1995, de 23 de noviembre, del ...</td>\n",
       "      <td>1995-11-24</td>\n",
       "      <td>Estatal</td>\n",
       "      <td>Jefatura del Estado</td>\n",
       "      <td>Ley Orgánica</td>\n",
       "      <td>1995-25444_chunk3</td>\n",
       "      <td>En segundo lugar, se ha afrontado la antinomia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1995-25444</td>\n",
       "      <td>https://www.boe.es/diario_boe/xml.php?id=BOE-A...</td>\n",
       "      <td>Ley Orgánica 10/1995, de 23 de noviembre, del ...</td>\n",
       "      <td>1995-11-24</td>\n",
       "      <td>Estatal</td>\n",
       "      <td>Jefatura del Estado</td>\n",
       "      <td>Ley Orgánica</td>\n",
       "      <td>1995-25444_chunk4</td>\n",
       "      <td>En cuarto lugar, y en consonancia con el objet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1995-25444</td>\n",
       "      <td>https://www.boe.es/diario_boe/xml.php?id=BOE-A...</td>\n",
       "      <td>Ley Orgánica 10/1995, de 23 de noviembre, del ...</td>\n",
       "      <td>1995-11-24</td>\n",
       "      <td>Estatal</td>\n",
       "      <td>Jefatura del Estado</td>\n",
       "      <td>Ley Orgánica</td>\n",
       "      <td>1995-25444_chunk5</td>\n",
       "      <td>En quinto lugar, se ha procurado avanzar en el...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                                url  \\\n",
       "0  1995-25444  https://www.boe.es/diario_boe/xml.php?id=BOE-A...   \n",
       "1  1995-25444  https://www.boe.es/diario_boe/xml.php?id=BOE-A...   \n",
       "2  1995-25444  https://www.boe.es/diario_boe/xml.php?id=BOE-A...   \n",
       "3  1995-25444  https://www.boe.es/diario_boe/xml.php?id=BOE-A...   \n",
       "4  1995-25444  https://www.boe.es/diario_boe/xml.php?id=BOE-A...   \n",
       "\n",
       "                                               title        date  \\\n",
       "0  Ley Orgánica 10/1995, de 23 de noviembre, del ...  1995-11-24   \n",
       "1  Ley Orgánica 10/1995, de 23 de noviembre, del ...  1995-11-24   \n",
       "2  Ley Orgánica 10/1995, de 23 de noviembre, del ...  1995-11-24   \n",
       "3  Ley Orgánica 10/1995, de 23 de noviembre, del ...  1995-11-24   \n",
       "4  Ley Orgánica 10/1995, de 23 de noviembre, del ...  1995-11-24   \n",
       "\n",
       "  legislative_origin           department          rang            text_id  \\\n",
       "0            Estatal  Jefatura del Estado  Ley Orgánica  1995-25444_chunk1   \n",
       "1            Estatal  Jefatura del Estado  Ley Orgánica  1995-25444_chunk2   \n",
       "2            Estatal  Jefatura del Estado  Ley Orgánica  1995-25444_chunk3   \n",
       "3            Estatal  Jefatura del Estado  Ley Orgánica  1995-25444_chunk4   \n",
       "4            Estatal  Jefatura del Estado  Ley Orgánica  1995-25444_chunk5   \n",
       "\n",
       "                                                text  \n",
       "0  JUAN CARLOS I\\nREY DE ESPAÑA\\nA todos los que ...  \n",
       "1  A partir de los distintos intentos de reforma ...  \n",
       "2  En segundo lugar, se ha afrontado la antinomia...  \n",
       "3  En cuarto lugar, y en consonancia con el objet...  \n",
       "4  En quinto lugar, se ha procurado avanzar en el...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split each text in the 'text' column into chunks recursively\n",
    "df_txt['chunks'] = df_txt['text'].apply(lambda x: recursive_text_splitter(x))\n",
    "\n",
    "# Create a new DataFrame with individual chunks and unique identifiers\n",
    "chunks_list = []\n",
    "for idx, row in df_txt.iterrows():\n",
    "    for i, chunk in enumerate(row['chunks']):\n",
    "        chunks_list.append({\n",
    "            'id': row['id'],\n",
    "            'url': row['url'],\n",
    "            'title': row['title'],\n",
    "            'date': row['date'],\n",
    "            'legislative_origin': row['legislative_origin'],\n",
    "            'department': row['department'],\n",
    "            'rang': row['rang'],\n",
    "            'text_id': f\"{row['id']}_chunk{i+1}\",\n",
    "            'text': chunk\n",
    "        })\n",
    "\n",
    "# Drop the 'chunks' column\n",
    "df_txt = df_txt.drop(columns=['chunks'])\n",
    "\n",
    "# Create a new DataFrame for the chunks\n",
    "splitted_df_v1 = pd.DataFrame(chunks_list)\n",
    "\n",
    "# Show\n",
    "splitted_df_v1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Length of file\n",
    "len(splitted_df_v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'google/gemma-2b-it'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarizer\n",
    "model = config['extract_core_topics_model']\n",
    "\n",
    "# Show model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1072cd8ce5744f8180c3970a5420b621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78cd82664a4f4e5abcff2ab4404eaf94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39a5ecfb54b455dbfd5dcf40dc83616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f67eaac3724c219a42dd2d20d38cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/888 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb547de5297b48a7b53e3a7b1f84e51a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb74b7f8ce64b1e84ba7fd6afaae477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f44540f1ffb14bbc973ec08de850163a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60acbe452c5f433bb605e5b73fb5c948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa43ff5d31441eaaa26b2963c75fd8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3b0ebf4b394218bc7578e4a84ab546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ffeec8a96740d4bce3c9bd94299325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summarizer\n",
    "model_id = config['extract_core_topics_model']\n",
    "\n",
    "# Quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id, \n",
    "    add_eos_token = True\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    quantization = bnb_config\n",
    "    device_map = \"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text using the specified LLM model\n",
    "def generate_text(prompt, max_length = 512, cuda = True):\n",
    "    \"\"\"\n",
    "    Generate text based on prompt and LLM model\n",
    "    Args:\n",
    "    - prompt (str): The input prompt for text generation.\n",
    "    - max_length (int): The maximum length of the generated text.\n",
    "    - cuda (bool): If True, use GPU for computation.\n",
    "\n",
    "    Returns:\n",
    "    - str: The generated text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Move model and input tensors to GPU if cuda is True\n",
    "    device = \"cuda\" if cuda and torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(input_ids, max_length = max_length, num_return_sequences = 1)\n",
    "\n",
    "    # Decode generated text\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Return\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Genera un resumen de las partes más importantes del siguiente texto, haciendo referencia continuada a los artículos mencionados y los puntos más relevantes'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get prompt\n",
    "extract_core_prompt = config[\"extract_core_prompt\"]\n",
    "\n",
    "# Show\n",
    "extract_core_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New object from previous splitting process\n",
    "splitted_df_v1_summary = splitted_df_v1.copy()\n",
    "\n",
    "# Create empty column\n",
    "splitted_df_v1_summary['text_summary'] = ''\n",
    "\n",
    "# Initialize a counter for iterations\n",
    "iteration_counter = 0\n",
    "\n",
    "# Loop by row to generate one summary per text\n",
    "for text_idx, text in tqdm(enumerate(splitted_df_v1_summary['text']), total = len(splitted_df_v1_summary)):\n",
    "    \n",
    "    # Prepend the prompt to the text\n",
    "    text_with_prompt = extract_core_prompt + text\n",
    "    \n",
    "    # Generate summary\n",
    "    summarized_text = generate_text(text)\n",
    "    \n",
    "    # Assign summary\n",
    "    splitted_df_v1_summary.at[text_idx, 'text_summary'] = summarized_text\n",
    "    \n",
    "    # Increment the iteration counter\n",
    "    iteration_counter += 1\n",
    "    \n",
    "    # Check if it's time to perform garbage collection\n",
    "    if iteration_counter % 1000 == 0:\n",
    "        # Clean memory\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "# Show\n",
    "splitted_df_v1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of file\n",
    "len(splitted_df_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save splitted & summarized data\n",
    "path = 'prepared_data/'\n",
    "csv_file_name_v4 = f'{path}splitted_input_gemma.csv'\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "splitted_df_v1_summary.to_csv(csv_file_name_v4, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA information\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# End time of notebook run\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Convert elapsed time to hours and minutes\n",
    "hours = int(elapsed_time // 3600)\n",
    "minutes = int((elapsed_time % 3600) // 60)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Time elapsed: {hours} hours and {minutes} minutes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
